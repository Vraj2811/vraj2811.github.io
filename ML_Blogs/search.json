[
  {
    "objectID": "blogs/SoftMax.html",
    "href": "blogs/SoftMax.html",
    "title": "Softmax Function",
    "section": "",
    "text": "Import Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport math\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\n\n\n\nFunction\n\\[ \\large\na_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }}\n\\]\n\ndef softmax(z):\n    ez = np.exp(z)\n    sm = ez/np.sum(ez)\n    return (sm)\n\n\n\nExample\n\ninput_array = np.array([1, 2, 3, 4])\nsoftmax_result = softmax(input_array)\n\ncustom_labels = ['z0', 'z1', 'z2', 'z3']\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\naxs[0].bar(np.arange(len(input_array)), input_array,\n           tick_label=custom_labels, color='b')\naxs[0].set_title('Input Array')\naxs[0].set_xlabel('Index (zi)')\naxs[0].set_ylabel('Value')\nfor i, v in enumerate(input_array):\n    axs[0].text(i, v, str(v), ha='center', va='bottom', fontsize=12)\n\naxs[1].bar(np.arange(len(softmax_result)), softmax_result,\n           tick_label=custom_labels, color='g')\naxs[1].set_title('Softmax Result')\naxs[1].set_xlabel('Index (zi)')\naxs[1].set_ylabel('Value')\nfor i, v in enumerate(softmax_result):\n    axs[1].text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nCost Function\n\\[\n  L(\\mathbf{a},y)=\\begin{cases}\n    -log(a_1), & \\text{if $y=1$}.\\\\\n        &\\vdots\\\\\n     -log(a_N), & \\text{if $y=N$}\n  \\end{cases}\n\\]\n\\[\nJ(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right]\n\\]\n\ndef loss(x, y):\n    a = softmax(x)\n    epsilon = 1e-15  # Small constant to avoid taking log(0)\n    return -math.log(a[y] + epsilon)\n\n\ndef cost_fxn(X, y):\n    m = X.shape[0]\n    cost = 0\n    for i in range(m):\n        cost += loss(X[i], y[i])\n\n    cost = cost / m\n    return cost\n\n\n\nDataset\n\ncenters = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\nX_train, y_train = make_blobs(\n    n_samples=2000, centers=centers, cluster_std=1.0, random_state=30)\n\n\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(X_train[:, 0], X_train[:, 1],\n                      c=y_train, cmap='viridis', marker='o', s=25)\n\nlegend_labels = [f'Cluster {i}' for i in range(len(centers))]\nplt.legend(handles=scatter.legend_elements()[\n           0], labels=legend_labels, title=\"Clusters\")\n\nplt.title(\"Generated Dataset with Four Clusters\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\n\nplt.show()\n\n\n\n\n\n\nModels\n\nmodel1 = Sequential(\n    [\n        Dense(25, activation='relu'),\n        Dense(15, activation='relu'),\n        Dense(4, activation='softmax')        # &lt;-- softmax activation here\n    ]\n)\nmodel1.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\nmodel1.fit(\n    X_train, y_train,\n    epochs=10\n)\n\nEpoch 1/10\n63/63 [==============================] - 1s 2ms/step - loss: 1.0357\nEpoch 2/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.4241\nEpoch 3/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.1823\nEpoch 4/10\n63/63 [==============================] - 0s 3ms/step - loss: 0.0980\nEpoch 5/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0683\nEpoch 6/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0532\nEpoch 7/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0426\nEpoch 8/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0369\nEpoch 9/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0329\nEpoch 10/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0300\n\n\n&lt;keras.src.callbacks.History at 0x1eeea31c040&gt;\n\n\n\nmodel2 = Sequential(\n    [\n        Dense(25, activation='relu'),\n        Dense(15, activation='relu'),\n        Dense(4, activation='linear')         # &lt;-- linear activation here\n    ]\n)\nmodel2.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\nmodel2.fit(\n    X_train, y_train,\n    epochs=10\n)\n\nEpoch 1/10\n63/63 [==============================] - 1s 2ms/step - loss: 1.0976\nEpoch 2/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.4926\nEpoch 3/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.2572\nEpoch 4/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.1406\nEpoch 5/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0897\nEpoch 6/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0672\nEpoch 7/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0547\nEpoch 8/10\n63/63 [==============================] - 0s 3ms/step - loss: 0.0476\nEpoch 9/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0422\nEpoch 10/10\n63/63 [==============================] - 0s 2ms/step - loss: 0.0381\n\n\n&lt;keras.src.callbacks.History at 0x1eeeb1a0ee0&gt;\n\n\n\n\nOutputs\n\np_preferred = model1.predict(X_train)\ndata = []\n\nfor i in range(5):\n    row = {\n        'Prediction': p_preferred[i],\n        'Category': np.argmax(p_preferred[i]),\n        'Max': np.max(p_preferred[i]),\n        'Min': np.min(p_preferred[i])\n    }\n    data.append(row)\n\ndf = pd.DataFrame(data)\n\nprint(df)\n\nprint(\"\")\n\ny_pred = np.argmax(p_preferred, axis=1)\ncorrect_predictions = np.sum(y_pred == y_train)\ntotal_predictions = len(y_train)\nerror_percentage = (1 - (correct_predictions / total_predictions)) * 100\nprint(\"Error percentage =\", error_percentage, \"%\")\n\n63/63 [==============================] - 0s 2ms/step\n                                               Prediction  Category       Max  \\\n0        [0.00076469, 0.0032128657, 0.981888, 0.01413442]         2  0.981888   \n1  [0.9984345, 0.0015346134, 2.1353391e-05, 9.585833e-06]         0  0.998434   \n2  [0.9823111, 0.017138032, 0.00038771704, 0.00016316074]         0  0.982311   \n3  [0.0013412955, 0.99340725, 0.004864373, 0.00038706747]         1  0.993407   \n4  [0.003443484, 0.00016099671, 0.9962613, 0.00013426131]         2  0.996261   \n\n        Min  \n0  0.000765  \n1  0.000010  \n2  0.000163  \n3  0.000387  \n4  0.000134  \n\nError percentage = 0.8499999999999952 %\n\n\n\np_preferred = model2.predict(X_train)\ndata = []\n\nfor i in range(5):\n    row = {\n        'Prediction': p_preferred[i],\n        'Category': np.argmax(p_preferred[i]),\n        'Max': np.max(p_preferred[i]),\n        'Min': np.min(p_preferred[i])\n    }\n    data.append(row)\n\ndf = pd.DataFrame(data)\n\nprint(df)\n\nprint(\"\")\n\ny_pred = np.argmax(p_preferred, axis=1)\ncorrect_predictions = np.sum(y_pred == y_train)\ntotal_predictions = len(y_train)\nerror_percentage = (1 - (correct_predictions / total_predictions)) * 100\nprint(\"Error percentage =\", error_percentage, \"%\")\n\n63/63 [==============================] - 0s 2ms/step\n                                         Prediction  Category       Max  \\\n0     [-4.3254666, 0.14574976, 3.842623, -0.401972]         2  3.842623   \n1     [6.7959185, 1.1274883, -5.037459, -2.7135649]         0  6.795918   \n2     [4.8487134, 1.3494179, -3.651421, -2.2844274]         0  4.848713   \n3  [-2.8908267, 4.823671, -0.023022205, -1.1583089]         1  4.823671   \n4     [-1.3442385, 0.3332066, 4.854504, -3.0173476]         2  4.854504   \n\n        Min  \n0 -4.325467  \n1 -5.037459  \n2 -3.651421  \n3 -2.890827  \n4 -3.017348  \n\nError percentage = 1.0499999999999954 %"
  },
  {
    "objectID": "blogs/Polynomial_Regression.html",
    "href": "blogs/Polynomial_Regression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math"
  },
  {
    "objectID": "blogs/Polynomial_Regression.html#dataset-y-1-x2",
    "href": "blogs/Polynomial_Regression.html#dataset-y-1-x2",
    "title": "Polynomial Regression",
    "section": "Dataset ( y = 1 + x^2 )",
    "text": "Dataset ( y = 1 + x^2 )\n\nx = np.arange(0, 20, 1)\ny = 1 + x**2\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\")\nplt.legend()\nplt.title(\"y=1+x^2\")\nplt.show()\n\n\n\n\n\nPredictions for Different Degrees\n\nDegree = 0\n\ndegree = 0\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nDegree = 1\n\ndegree = 1\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nDegree = 2\n\ndegree = 2\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blogs/Polynomial_Regression.html#dataset-2-y-cosx2",
    "href": "blogs/Polynomial_Regression.html#dataset-2-y-cosx2",
    "title": "Polynomial Regression",
    "section": "Dataset 2 ( y = Cos(x/2) )",
    "text": "Dataset 2 ( y = Cos(x/2) )\n\nx = np.arange(0,20,1)\ny = np.cos(x/2)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\")\nplt.legend()\nplt.title(\"y=Cos(x/2)\")\nplt.show()\n\n\n\n\n\nPredictions for Different Degrees\n\nDegree = 1\n\ndegree = 1\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nDegree = 4\n\ndegree = 4\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nDegree = 9\n\ndegree = 9\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blogs/Logistic_Regression.html",
    "href": "blogs/Logistic_Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom matplotlib import cm\n\n\n\nSome Plotting Fxn\n\ndef soup_bowl(x_train, y_train, w, b):\n    wx, by = np.meshgrid(np.linspace(-6, 12, 50),\n                         np.linspace(10, -20, 40))\n    points = np.c_[wx.ravel(), by.ravel()]\n    cost = np.zeros(points.shape[0])\n\n    for i in range(points.shape[0]):\n        w_i, b_i = points[i]\n        cost[i] = cost_fxn(x_train, y_train, w_i, b_i)\n\n    cost = cost.reshape(wx.shape)\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(1, 1, 1, projection='3d')\n    ax.plot_surface(wx, by, cost, alpha=0.6, cmap=cm.jet)\n    ax.set_xlabel('w', fontsize=20)\n    ax.set_ylabel('b', fontsize=20)\n    ax.set_zlabel(\"Cost\", rotation=90, fontsize=20)\n    ax.set_title('\"Logistic\" Squared Error Cost vs (w, b)', fontsize=20)\n    cscat = ax.scatter(w, b, s=100, color='red')\n    plt.tight_layout()\n\n    plt.show()\n\n\n\nDataset\n\nx_train = np.array([0, 1, 2, 3, 4, 5]).reshape(-1, 1)\ny_train = np.array([0,  0, 0, 1, 1, 1])\n\nw = np.array([5])\nb = 10\n\n\npos = y_train == 1\nneg = y_train == 0\n\nplt.figure(figsize=(6, 4))\n\nplt.scatter(x_train[pos], y_train[pos],\n            marker='x', s=80, c='red', label=\"y=1\")\nplt.scatter(x_train[neg], y_train[neg], marker='o',\n            s=100, label=\"y=0\", facecolors='blue', edgecolors='black', linewidth=1)\nplt.ylim(-0.08, 1.1)\nplt.ylabel('y', fontsize=12)\nplt.xlabel('x', fontsize=12)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nSigmoid Fxn\n\\[g(z) = \\frac{1}{1+e^{-z}}\\]\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nz = np.linspace(-8, 8, 100)\n\nsigmoid_values = sigmoid(z)\n\nplt.figure(figsize=(6, 4))\nplt.plot(z, sigmoid_values, label='Sigmoid Function')\nplt.xlabel('z')\nplt.ylabel('sigmoid(z)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nFinding Function f_wb\n\\[ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\]\n\ndef fxn(x, w, b):\n    f_wb = sigmoid(np.dot(x, w) + b)\n    return f_wb\n\n\n\nDecision Boundary\n\\[\\mathbf{w} \\cdot \\mathbf{x} = w_0 x_0 + w_1 x_1 = 0\\]\n\nDataset\n\nx_db = np.array([[0.5, 1.5], [1, 1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_db = np.array([0, 0, 0, 1, 1, 1]).reshape(-1, 1)\n\n\nx0 = np.arange(0, 6)\n\nx1 = 3 - x0\nfig, ax = plt.subplots(1, 1, figsize=(5, 4))\nax.plot(x0, x1, c=\"b\")\nax.axis([0, 4, 0, 3.5])\n\nax.fill_between(x0, x1, alpha=0.2)\n\npos = y_db == 1\nneg = y_db == 0\npos = pos.reshape(-1,)\nneg = neg.reshape(-1,)\n\nplt.scatter(x_db[neg, 0], x_db[neg, 1], marker='o', s=80,\n            label=\"neg_label\", facecolors='none', edgecolors=\"blue\", lw=3)\nplt.scatter(x_db[pos, 0], x_db[pos, 1], marker='x',\n            s=80, c='red', label=\"pos_label\")\nplt.show()\nplt.show()\n\n\n\n\n\n\n\nLoss Fxn\n\\[\n  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n  \\end{cases}\n\\]\n\\[= -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\]\n\ndef loss(x, y, w, b):\n    a = fxn(x, w, b)\n    epsilon = 1e-15  # Small constant to avoid taking log(0)\n\n    loss = -y * math.log(a + epsilon) - (1 - y) * math.log(1 - a + epsilon)\n    return loss\n\n\n\nCost Fxn\n\\[ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right]\\]\n\ndef cost_fxn(X, y, w, b):\n\n    m = X.shape[0]\n    cost = 0\n    for i in range(m):\n        cost += loss(X[i], y[i], w, b)\n\n    cost = cost / m\n    return cost\n\n\n\nSome Plots -&gt;\n\nFinal w, b\n\nif len(w) == 1:\n    fxn1 = fxn(x_train, w, b)\n\n    plt.scatter(x_train, y_train, color=\"red\")\n    plt.plot(x_train, fxn1)\n    plt.show()\n\nsoup_bowl(x_train, y_train, w, b)\n\n\n\n\nC:\\Users\\vrajs\\AppData\\Local\\Temp\\ipykernel_16560\\2055154098.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  loss = -y * math.log(a + epsilon) - (1 - y) * math.log(1 - a + epsilon)\n\n\n\n\n\n\n\n\nFinding dJ/dw and dJ/db\n\\[\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n\\end{align*}\\]\n\ndef compute_gradient(x, y, w, b):\n    dj_dw = 0\n    dj_db = 0\n    m = x.shape[0]\n    a = fxn(x, w, b) - y\n    dj_dw = (np.dot(a, x)) / m\n    dj_db = np.sum(a) / m\n    return dj_dw, dj_db\n\n\n\nGradient Descent\n\\[\\begin{align*}\n&\\text{repeat until convergence:} \\; \\lbrace \\\\\n&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j := 0..n-1} \\\\\n&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n&\\rbrace\n\\end{align*}\\]\n\ndef gradient_descent(x, y, w, b, alpha, num_iters):\n    J_history = []\n    p_history = []\n\n    for i in range(num_iters+1):\n        dj_dw, dj_db = compute_gradient(x, y, w, b)\n        b = b - alpha * dj_db\n        w = w - alpha * dj_dw\n        J_history.append(cost_fxn(x, y, w, b))\n        p_history.append([w, b])\n        if i % math.ceil(num_iters/10) == 0:\n            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}, w: {w}, b:{b}\")\n\n    return w, b, J_history, p_history\n\n\niterations = 10000\ntmp_alpha = 1.0e-1\n\n\nw_final, b_final, J_hist, p_hist = gradient_descent(\n    x_train, y_train, w, b, tmp_alpha, iterations)\nprint(f\"(w,b) found by gradient descent: ({w_final},{b_final})\")\n\n\nf_wb = fxn(x_train, w_final, b_final)\nprint(\"Cost is\", cost_fxn(x_train, y_train, w_final, b_final))\n\nIteration    0: Cost 7.45e+00, w: [4.95000001], b:9.950000761764102\nIteration 1000: Cost 1.32e-01, w: [1.94357846], b:-4.532187811486597\nIteration 2000: Cost 8.42e-02, w: [2.71585819], b:-6.535471646001672\nIteration 3000: Cost 6.46e-02, w: [3.22521954], b:-7.834466336781398\nIteration 4000: Cost 5.30e-02, w: [3.62055449], b:-8.83606124058392\nIteration 5000: Cost 4.51e-02, w: [3.94791911], b:-9.662597001748475\nIteration 6000: Cost 3.92e-02, w: [4.22882495], b:-10.37034842110977\nIteration 7000: Cost 3.48e-02, w: [4.47546538], b:-10.990900553853033\nIteration 8000: Cost 3.12e-02, w: [4.69557844], b:-11.544161480692926\nIteration 9000: Cost 2.83e-02, w: [4.89444965], b:-12.043663029887693\nIteration 10000: Cost 2.59e-02, w: [5.07588043], b:-12.499101969930441\n(w,b) found by gradient descent: ([5.07588043],-12.499101969930441)\nCost is 0.025934093960807036\n\n\n\n\nSome Plots -&gt;\n\nFinal w, b\n\nif len(w) == 1:\n    fxn2 = fxn(x_train, w_final, b_final)\n\n    plt.scatter(x_train, y_train, color=\"red\")\n    plt.plot(x_train, fxn2)\n    plt.show()\n    \nsoup_bowl(x_train, y_train, w_final, b_final)\n\n\n\n\nC:\\Users\\vrajs\\AppData\\Local\\Temp\\ipykernel_16560\\2055154098.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  loss = -y * math.log(a + epsilon) - (1 - y) * math.log(1 - a + epsilon)\n\n\n\n\n\n\n\n\nRegularized Linear Regression\n\nFinding Cost Fxn\n\\[J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\]\n\ndef cost_fxn_regularized(X, y, w, b, lambda_ = 1):\n    cost=cost_fxn(X, y, w, b)\n    cost += np.sum(w ** 2)\n    return cost\n\n\n\nFinding dJ/dw and dJ/db\n\\[\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n\\end{align*}\\]\n\ndef compute_gradient_regularized(X, y, w, b, lambda_):\n    m = X.shape[0]\n    dj_dw, dj_db = compute_gradient(X, y, w, b)\n\n    dj_dw += (lambda_ / m) * w\n\n    return dj_db, dj_dw"
  },
  {
    "objectID": "blogs/Linear_Regression_With_Multiple_Variable.html",
    "href": "blogs/Linear_Regression_With_Multiple_Variable.html",
    "title": "Linear Regression With Multiple Variable",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\n\nDataset\n\nx_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\ny_train = np.array([460, 232, 178])\nb = -100\nw = np.array([10, -10, 50, -20])\n\n\n\nFinding Function f_wb\n\\[ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\] \\[ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b   \\]\n\ndef fxn(x, w, b):\n    f_wb = np.dot(x, w) + b\n\n    return f_wb\n\n\n\nFinding Cost Function\n\\[J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\]\n\ndef cost_fxn(x, y, w, b):\n    m = x.shape[0]\n\n    f_wb = fxn(x, w, b)\n    cost = (f_wb - y) ** 2\n    total_cost = (1 / (2 * m)) * np.sum(cost)\n\n    return total_cost\n\n\n\nFinding dJ/dw and dJ/db\n\\[\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\\\\n\\]\n\ndef compute_gradient(x, y, w, b):\n    m = x.shape[0]\n    a = fxn(x, w, b) - y\n    dj_dw = (np.dot(a, x))/m\n    dj_db = np.sum(a)/m\n    return dj_dw, dj_db\n\n\n\nGradient Descent\n\\[\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  \\; & \\text{for j = 0..n-1}\\newline\n&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}\\]\n\ndef gradient_descent(x, y, w, b, alpha, num_iters):\n    J_history = []\n    p_history = []\n\n    for i in range(num_iters + 1):\n        dj_dw, dj_db = compute_gradient(x, y, w, b)\n        w = w - alpha * dj_dw\n        b = b - alpha * dj_db\n\n        cost = cost_fxn(x, y, w, b)\n        J_history.append(cost)\n        p_history.append((w, b))\n\n        if i % math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4}: Cost {cost:.2e}, w: {w}, b: {b}\")\n\n    return w, b, J_history, p_history\n\n\niterations = 100000\ntmp_alpha = 5.0e-7\n\nw_final, b_final, J_hist, p_hist = gradient_descent(\n    x_train, y_train, w, b, tmp_alpha, iterations)\nprint(f\"(w,b) found by gradient descent: {w_final},{b_final}\")\n\n\nf_wb = fxn(x_train, w_final, b_final)\nprint(\"Cost is\", cost_fxn(x_train, y_train, w_final, b_final))\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\nax.set_title(\"Cost vs. iteration (end)\")\nax.set_ylabel('Cost')\nax.set_xlabel('iteration step')\nplt.show()\n\nIteration    0: Cost 3.85e+06, w: [ -1.03240533 -10.02538967  49.99110867 -20.27834   ], b: -100.00670833333334\nIteration 10000: Cost 8.71e+03, w: [  0.58117979 -10.02160299  50.21100127 -13.2564183 ], b: -99.75760289212182\nIteration 20000: Cost 4.63e+03, w: [  0.467721   -10.00429679  50.3020335   -8.78158635], b: -99.59764948351305\nIteration 30000: Cost 2.95e+03, w: [ 0.39492336 -9.97570356 50.311448   -5.91048217], b: -99.49414563632797\nIteration 40000: Cost 2.26e+03, w: [ 0.34819099 -9.93989568 50.26860068 -4.06742547], b: -99.42682903727315\nIteration 50000: Cost 1.97e+03, w: [ 0.31816743 -9.89948366 50.19230962 -2.88338545], b: -99.38271084836177\nIteration 60000: Cost 1.85e+03, w: [ 0.29885486 -9.85614089 50.09463778 -2.12180075], b: -99.35346477833464\nIteration 70000: Cost 1.80e+03, w: [ 0.28640849 -9.81094006 49.98331786 -1.63102743], b: -99.33375338803842\nIteration 80000: Cost 1.77e+03, w: [ 0.27836366 -9.76456876 49.86330674 -1.3138565 ], b: -99.32015519486049\nIteration 90000: Cost 1.76e+03, w: [ 0.27314044 -9.71746776 49.73778185 -1.10797371], b: -99.31047690994058\nIteration 100000: Cost 1.75e+03, w: [ 0.26972607 -9.66991956 49.60877999 -0.97343345], b: -99.3033125586677\n(w,b) found by gradient descent: [ 0.26972607 -9.66991956 49.60877999 -0.97343345],-99.3033125586677\nCost is 1753.0909002940225"
  },
  {
    "objectID": "blogs/Learning_Tensorflow.html",
    "href": "blogs/Learning_Tensorflow.html",
    "title": "Basic Tensorflow",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\n\nWARNING:tensorflow:From C:\\Users\\vrajs\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead."
  },
  {
    "objectID": "blogs/Learning_Tensorflow.html#neuron-with-linear-regression-model",
    "href": "blogs/Learning_Tensorflow.html#neuron-with-linear-regression-model",
    "title": "Basic Tensorflow",
    "section": "Neuron with Linear Regression Model",
    "text": "Neuron with Linear Regression Model\n\nDataset\n\nX_train = np.array([[1.0], [2.0], [3.0]], dtype=np.float32)\nY_train = np.array([[300.0], [500.0], [700.0]], dtype=np.float32)\n\nfig, ax = plt.subplots(1, 1)\nax.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")\nax.legend(fontsize='large')\nax.set_ylabel('Price', fontsize='large')\nax.set_xlabel('Size', fontsize='large')\nplt.show()\n\n\n\n\n\n\nLinear Regression Model\n\nmodel = Sequential(\n    [\n        tf.keras.layers.Dense(1, input_dim=1,  activation='linear', name='L1')\n    ]\n)\n\nWARNING:tensorflow:From C:\\Users\\vrajs\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n L1 (Dense)                  (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2 (8.00 Byte)\nTrainable params: 2 (8.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nInstantiation of weights\n\nw, b = model.get_weights()      # Random Generation\nprint(f\"w = {w}, b={b}\")\n\nw = [[1.3145238]], b=[0.]\n\n\n\na = model(X_train)              # Random Output\nprint(a)\n\ntf.Tensor(\n[[1.3145238]\n [2.6290476]\n [3.9435716]], shape=(3, 1), dtype=float32)\n\n\n\n\nSetting the weights\n\nset_w = np.array([[200]])\nset_b = np.array([100])\n\nmodel.set_weights([set_w, set_b])\nprint(model.get_weights())\n\n[array([[200.]], dtype=float32), array([100.], dtype=float32)]\n\n\n\n\nComparing Linear Model Framework and Manual Implementation\n\\[ f_{\\mathbf{w},b}(x^{(i)}) = \\mathbf{w}\\cdot x^{(i)} + b \\]\n\nprediction_tf = model(X_train)\nprediction_np = np.dot(X_train, set_w) + set_b\n\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(X_train, prediction_tf, label='Model Output',\n         color='blue', linewidth=4)\nplt.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\", s=200)\nplt.xlabel('X_train')\nplt.ylabel('Prediction')\nplt.title('Tensorflow Prediction')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(X_train, prediction_np, label='Model Output',\n         color='blue', linewidth=4)\nplt.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\", s=200)\nplt.xlabel('X_train')\nplt.ylabel('Prediction')\nplt.title('Numpy Prediction')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blogs/Learning_Tensorflow.html#neuron-with-logistic-regression-model",
    "href": "blogs/Learning_Tensorflow.html#neuron-with-logistic-regression-model",
    "title": "Basic Tensorflow",
    "section": "Neuron with Logistic Regression Model",
    "text": "Neuron with Logistic Regression Model\n\nDataset\n\nX_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1, 1)\nY_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1, 1)\n\npos = Y_train == 1\nneg = Y_train == 0\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c='red', label=\"y=1\")\nax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", lw=3)\n\nax.set_ylim(-0.08, 1.1)\nax.set_ylabel('y', fontsize=12)\nax.set_xlabel('x', fontsize=12)\nax.set_title('one variable plot')\nax.legend(fontsize=12)\nplt.show()\n\n\n\n\n\n\nLogistic Regression Model\n\nmodel = Sequential(\n    [\n        tf.keras.layers.Dense(1, input_dim=1,  activation='sigmoid')\n    ]\n)\n\n\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2 (8.00 Byte)\nTrainable params: 2 (8.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nInstantiation of weights\n\nw, b = model.get_weights()      # Random Generation\nprint(f\"w = {w}, b={b}\")\n\nw = [[0.3190199]], b=[0.]\n\n\n\na = model(X_train)              # Random Output\nprint(a)\n\ntf.Tensor(\n[[0.5       ]\n [0.57908535]\n [0.6543102 ]\n [0.72253275]\n [0.78178173]\n [0.8313324 ]], shape=(6, 1), dtype=float32)\n\n\n\n\nSetting the weights\n\nset_w = np.array([[2]])\nset_b = np.array([-4.5])\n\nmodel.set_weights([set_w, set_b])\nprint(model.get_weights())\n\n[array([[2.]], dtype=float32), array([-4.5], dtype=float32)]\n\n\n\n\nComparing Logistic Model Framework and Manual Implementation\n\\[ f_{\\mathbf{w},b}(x^{(i)}) = g(\\mathbf{w}x^{(i)} + b) \\]\n\nprediction_tf = model(X_train)\nprediction_np = 1/(1+np.exp(-(np.dot(X_train, set_w) + set_b)))\n\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(X_train, prediction_tf, label='Model Output',\n         color='blue', linewidth=4)\nplt.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\", s=200)\nplt.xlabel('X_train')\nplt.ylabel('Prediction')\nplt.title('Tensorflow Prediction')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(X_train, prediction_np, label='Model Output',\n         color='blue', linewidth=4)\nplt.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\", s=200)\nplt.xlabel('X_train')\nplt.ylabel('Prediction')\nplt.title('Numpy Prediction')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blogs/Handwritten_DIgit_Recognition.html",
    "href": "blogs/Handwritten_DIgit_Recognition.html",
    "title": "Handwritten Digit Recognition",
    "section": "",
    "text": "Import Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n\n\nDataset\n\nX = np.load(\"data/X.npy\")\ny = np.load(\"data/y.npy\")\n\n\nprint('The shape of X is:', (X.shape))\nprint('The shape of y is:', (y.shape))\n\nprint('The shape of X[0] is:', (X[0].shape))\nprint('Value of y[0] is:', y[0])\n\nThe shape of X is: (5000, 400)\nThe shape of y is: (5000, 1)\nThe shape of X[0] is: (400,)\nValue of y[0] is: [0]\n\n\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8, 8, figsize=(10, 10))\nfig.tight_layout(rect=[0, 0.0, 1, 0.91])\n\nfor i, ax in enumerate(axes.flat):\n    random_index = np.random.randint(m)\n    X_random_reshaped = X[random_index].reshape((20, 20)).T\n\n    ax.imshow(X_random_reshaped, cmap='gray')\n    ax.set_title(y[random_index, 0], fontsize=19)\n    ax.set_axis_off()\n    fig.suptitle(\"Label, image\", fontsize=19)\n\n\n\n\n\n\nModel\n\ntf.random.set_seed(1234)\n\nmodel = Sequential(\n    [\n        tf.keras.Input(shape=(400,)),\n        Dense(units=25, activation='relu', name='L1'),\n        Dense(units=15, activation='relu', name='L2'),\n        Dense(units=10, activation='linear', name='L3'),\n\n    ], name=\"my_model\"\n)\n\nWARNING:tensorflow:From C:\\Users\\vrajs\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\n\n\n\nmodel.summary()\n\nModel: \"my_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n L1 (Dense)                  (None, 25)                10025     \n                                                                 \n L2 (Dense)                  (None, 15)                390       \n                                                                 \n L3 (Dense)                  (None, 10)                160       \n                                                                 \n=================================================================\nTotal params: 10575 (41.31 KB)\nTrainable params: 10575 (41.31 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n[layer1, layer2, layer3] = model.layers\n\nW1, b1 = layer1.get_weights()\nW2, b2 = layer2.get_weights()\nW3, b3 = layer3.get_weights()\nprint(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\nprint(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\nprint(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")\n\nW1 shape = (400, 25), b1 shape = (25,)\nW2 shape = (25, 15), b2 shape = (15,)\nW3 shape = (15, 10), b3 shape = (10,)\n\n\n\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n)\n\nhistory = model.fit(\n    X, y,\n    epochs=40\n)\n\nEpoch 1/40\nWARNING:tensorflow:From C:\\Users\\vrajs\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n157/157 [==============================] - 1s 2ms/step - loss: 1.8278\nEpoch 2/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.7689\nEpoch 3/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.4704\nEpoch 4/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.3696\nEpoch 5/40\n157/157 [==============================] - 0s 3ms/step - loss: 0.3148\nEpoch 6/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.2759\nEpoch 7/40\n157/157 [==============================] - 0s 3ms/step - loss: 0.2503\nEpoch 8/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.2280\nEpoch 9/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.2137\nEpoch 10/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1947\nEpoch 11/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1841\nEpoch 12/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1706\nEpoch 13/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1608\nEpoch 14/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1508\nEpoch 15/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1410\nEpoch 16/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1352\nEpoch 17/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1246\nEpoch 18/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1189\nEpoch 19/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1104\nEpoch 20/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1043\nEpoch 21/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1003\nEpoch 22/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0953\nEpoch 23/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0896\nEpoch 24/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0839\nEpoch 25/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0800\nEpoch 26/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0758\nEpoch 27/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0725\nEpoch 28/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0666\nEpoch 29/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0636\nEpoch 30/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0614\nEpoch 31/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0561\nEpoch 32/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0547\nEpoch 33/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0511\nEpoch 34/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0467\nEpoch 35/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0444\nEpoch 36/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0434\nEpoch 37/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0380\nEpoch 38/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0375\nEpoch 39/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0365\nEpoch 40/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0330\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(4, 3))\nax.plot(history.history['loss'], label='loss')\nax.set_ylim([0, 1.75])\nax.set_xlabel('Epoch')\nax.set_ylabel('loss (cost)')\nax.legend()\nax.grid(True)\nplt.show()\n\n\n\n\n\n\nPrediction\n\nX[1015]                 # This is an image of 2\nfig, ax = plt.subplots(1, 1, figsize=(0.5, 0.5))\nfig.tight_layout(rect=[0, 0.0, 1, 0.91])\nX_reshaped = X[1015].reshape((20, 20)).T\nax.set_axis_off()\nax.imshow(X_reshaped, cmap='gray')\nplt.show()\n\nC:\\Users\\vrajs\\AppData\\Local\\Temp\\ipykernel_18392\\2281255842.py:3: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations.\n  fig.tight_layout(rect=[0, 0.0, 1, 0.91])\n\n\n\n\n\n\nprediction = model.predict(X[1015].reshape(1, 400))          # prediction\n\nprint(f\"Prediction: \\n{prediction}\")\nprint(f\"Index: {np.argmax(prediction)}\")\n\n1/1 [==============================] - 0s 94ms/step\nPrediction: \n[[-8.457523   1.3707368  4.68359   -2.3333073 -9.920081  -7.871964\n  -7.7648544  0.6922297 -5.3786387 -5.3329587]]\nIndex: 2\n\n\n\nprediction = model.predict(X)\nyhat = np.argmax(prediction, axis=1)\ndoo = yhat != y[:, 0]\nidxs = np.where(yhat != y[:, 0])[0]\nprint( f\"{len(idxs)} errors out of {len(X)} images\")\n    \n\n157/157 [==============================] - 0s 2ms/step\n19 errors out of 5000 images"
  },
  {
    "objectID": "blogs/Collaborative_Filtering_Recommender_Systems.html",
    "href": "blogs/Collaborative_Filtering_Recommender_Systems.html",
    "title": "Content Filtering Recommender System",
    "section": "",
    "text": "Import Libraries\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom numpy import loadtxt\nimport pandas as pd\n\nWARNING:tensorflow:From C:\\Users\\vrajs\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\n\n\n\nDataset\n\nfile = open('data/small_movies_X.csv', 'rb')\nX = loadtxt(file, delimiter=\",\")\nfile = open('data/small_movies_W.csv', 'rb')\nW = loadtxt(file, delimiter=\",\")\nfile = open('data/small_movies_b.csv', 'rb')\nb = loadtxt(file, delimiter=\",\")\nb = b.reshape(1, -1)\nfile = open('data/small_movies_Y.csv', 'rb')\nY = loadtxt(file, delimiter=\",\")\nfile = open('data/small_movies_R.csv', 'rb')\nR = loadtxt(file, delimiter=\",\")\n\nmovieList_df = pd.read_csv('data/small_movie_list.csv',\n                           header=0, index_col=0,  delimiter=',', quotechar='\"')\nmovieList = movieList_df[\"title\"].to_list()\n\nnum_movies, num_features = X.shape\nnum_users, _ = W.shape\n\n\nprint(\"Y\", Y.shape)\nprint(\"R\", R.shape)\nprint(\"X\", X.shape)\nprint(\"W\", W.shape)\nprint(\"b\", b.shape)\nprint(\"num_features\", num_features)\nprint(\"num_movies\",   num_movies)\nprint(\"num_users\",    num_users)\n\nY (4778, 443)\nR (4778, 443)\nX (4778, 10)\nW (443, 10)\nb (1, 443)\nnum_features 10\nnum_movies 4778\nnum_users 443\n\n\n\nmean = np.mean(Y[152, R[152, :].astype(bool)])\nprint(f\"Average rating for movie 153 : {mean:0.3f} / 5\")\n\nAverage rating for movie 153 : 1.833 / 5\n\n\n\n\nCost Function\n\\[\nJ =  \\left[ \\frac{1}{2}\\sum_{(i,j):r(i,j)=1}(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right]\n+ \\underbrace{\\left[\n\\frac{\\lambda}{2}\n\\sum_{j=0}^{n_u-1}\\sum_{k=0}^{n-1}(\\mathbf{w}^{(j)}_k)^2\n+ \\frac{\\lambda}{2}\\sum_{i=0}^{n_m-1}\\sum_{k=0}^{n-1}(\\mathbf{x}_k^{(i)})^2\n\\right]}_{regularization}\n\\]\n\\[\n= \\left[ \\frac{1}{2}\\sum_{j=0}^{n_u-1} \\sum_{i=0}^{n_m-1}r(i,j)*(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right]\n+\\text{regularization}\n\\]\n\ndef cost_fxn(X, W, b, Y, R, lambda_):\n\n    # diff = np.dot(X, W.T) + b - Y\n    # squared_error = np.square(diff)\n\n    # squared_error *= R\n\n    # J = 0.5 * np.sum(squared_error) + (lambda_ / 2) * \\\n    #     (np.sum(np.square(W)) + np.sum(np.square(X)))\n\n    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * \\\n        (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n    return J\n\n\nJ = cost_fxn(X, W, b, Y, R, 0)\nprint(f\"Cost: {J:0.2f}\")\n\nJ = cost_fxn(X, W, b, Y, R, 1.5)\nprint(f\"Cost (with regularization): {J:0.2f}\")\n\nCost: 270821.25\nCost (with regularization): 306504.87\n\n\n\n\nMy Ratings\n\nmy_ratings = np.zeros(num_movies)\n\nmy_ratings[2700] = 5    # Toy Story 3 (2010)\nmy_ratings[2609] = 2    # Persuasion (2007)\nmy_ratings[929] = 5     # Lord of the Rings: The Return of the King, The\nmy_ratings[246] = 5     # Shrek (2001)\nmy_ratings[2716] = 3    # Inception\nmy_ratings[1150] = 5    # Incredibles, The (2004)\nmy_ratings[382] = 2     # Amelie (Fabuleux destin d'Amélie Poula\nmy_ratings[366] = 5     # Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\nmy_ratings[622] = 5     # Harry Potter and the Chamber of Secrets (2002)\nmy_ratings[988] = 3     # Eternal Sunshine of the Spotless Mind (2004)\nmy_ratings[2925] = 1    # Louis Theroux: Law & Disorder (2008)\nmy_ratings[2937] = 1    # Nothing to Declare (Rien à déclarer)\nmy_ratings[793] = 5     # Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n\nmy_rated = [i for i in range(len(my_ratings)) if my_ratings[i] &gt; 0]\n\nprint('\\nNew user ratings:\\n')\nfor i in range(len(my_ratings)):\n    if my_ratings[i] &gt; 0:\n        print(f'Rated {my_ratings[i]} for {movieList_df.loc[i,\"title\"]}')\n\nY = np.c_[my_ratings, Y]\n\nR = np.c_[(my_ratings != 0).astype(int), R] \n\n\nNew user ratings:\n\nRated 5.0 for Shrek (2001)\nRated 5.0 for Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\nRated 2.0 for Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nRated 5.0 for Harry Potter and the Chamber of Secrets (2002)\nRated 5.0 for Pirates of the Caribbean: The Curse of the Black Pearl (2003)\nRated 5.0 for Lord of the Rings: The Return of the King, The (2003)\nRated 3.0 for Eternal Sunshine of the Spotless Mind (2004)\nRated 5.0 for Incredibles, The (2004)\nRated 2.0 for Persuasion (2007)\nRated 5.0 for Toy Story 3 (2010)\nRated 3.0 for Inception (2010)\nRated 1.0 for Louis Theroux: Law & Disorder (2008)\nRated 1.0 for Nothing to Declare (Rien à déclarer) (2010)\n\n\n\nYmean = (np.sum(Y*R, axis=1)/(np.sum(R, axis=1)+1e-12)).reshape(-1, 1)\nYnorm = Y - np.multiply(Ymean, R)\n\n\n\nTraining the Model\n\nnum_movies, num_users = Y.shape\nnum_features = 100\n\ntf.random.set_seed(1234)\nW = tf.Variable(tf.random.normal(\n    (num_users,  num_features), dtype=tf.float64),  name='W')\nX = tf.Variable(tf.random.normal(\n    (num_movies, num_features), dtype=tf.float64),  name='X')\nb = tf.Variable(tf.random.normal(\n    (1,          num_users),   dtype=tf.float64),  name='b')\n\noptimizer = keras.optimizers.Adam(learning_rate=1e-1)\n\n\niterations = 201\nlambda_ = 1\n\nfor iter in range(iterations):\n    with tf.GradientTape() as tape:\n        cost_value = cost_fxn(X, W, b, Ynorm, R, lambda_)\n\n    grads = tape.gradient(cost_value, [X, W, b])\n\n    optimizer.apply_gradients(zip(grads, [X, W, b]))\n\n    if iter % 20 == 0:\n        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")\n\nTraining loss at iteration 0: 2321191.3\nTraining loss at iteration 20: 136169.3\nTraining loss at iteration 40: 51863.7\nTraining loss at iteration 60: 24599.0\nTraining loss at iteration 80: 13630.6\nTraining loss at iteration 100: 8487.7\nTraining loss at iteration 120: 5807.8\nTraining loss at iteration 140: 4311.6\nTraining loss at iteration 160: 3435.3\nTraining loss at iteration 180: 2902.1\nTraining loss at iteration 200: 2566.6\n\n\n\n\nPredictions\n\np = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n\npm = p + Ymean\n\nmy_predictions = pm[:, 0]\n\nix = tf.argsort(my_predictions, direction='DESCENDING')\n\nfor i in range(17):\n    j = ix[i]\n    if j not in my_rated:\n        print(\n            f'Predicting rating {my_predictions[j]:0.2f} for movie {movieList[j]}')\n\nprint('\\n\\nOriginal vs Predicted ratings:\\n')\nfor i in range(len(my_ratings)):\n    if my_ratings[i] &gt; 0:\n        print(\n            f'Original {my_ratings[i]}, Predicted {my_predictions[i]:0.2f} for {movieList[i]}')\n\nPredicting rating 4.49 for movie My Sassy Girl (Yeopgijeogin geunyeo) (2001)\nPredicting rating 4.48 for movie Martin Lawrence Live: Runteldat (2002)\nPredicting rating 4.48 for movie Memento (2000)\nPredicting rating 4.47 for movie Delirium (2014)\nPredicting rating 4.47 for movie Laggies (2014)\nPredicting rating 4.47 for movie One I Love, The (2014)\nPredicting rating 4.47 for movie Particle Fever (2013)\nPredicting rating 4.45 for movie Eichmann (2007)\nPredicting rating 4.45 for movie Battle Royale 2: Requiem (Batoru rowaiaru II: Chinkonka) (2003)\nPredicting rating 4.45 for movie Into the Abyss (2011)\n\n\nOriginal vs Predicted ratings:\n\nOriginal 5.0, Predicted 4.90 for Shrek (2001)\nOriginal 5.0, Predicted 4.84 for Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\nOriginal 2.0, Predicted 2.13 for Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\nOriginal 5.0, Predicted 4.88 for Harry Potter and the Chamber of Secrets (2002)\nOriginal 5.0, Predicted 4.87 for Pirates of the Caribbean: The Curse of the Black Pearl (2003)\nOriginal 5.0, Predicted 4.89 for Lord of the Rings: The Return of the King, The (2003)\nOriginal 3.0, Predicted 3.00 for Eternal Sunshine of the Spotless Mind (2004)\nOriginal 5.0, Predicted 4.90 for Incredibles, The (2004)\nOriginal 2.0, Predicted 2.11 for Persuasion (2007)\nOriginal 5.0, Predicted 4.80 for Toy Story 3 (2010)\nOriginal 3.0, Predicted 3.00 for Inception (2010)\nOriginal 1.0, Predicted 1.41 for Louis Theroux: Law & Disorder (2008)\nOriginal 1.0, Predicted 1.26 for Nothing to Declare (Rien à déclarer) (2010)"
  },
  {
    "objectID": "blogs/Anomaly_Detection.html",
    "href": "blogs/Anomaly_Detection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Import Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nDataset\n\nX_train = np.load(\"data/X_train.npy\")\nX_test = np.load(\"data/X_test.npy\")\ny_test = np.load(\"data/y_test.npy\")\n\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n\n(307, 2)\n(307, 2)\n(307,)\n\n\n\nplt.scatter(X_train[:, 0], X_train[:, 1], marker='x', c='b')\n\nplt.title(\"The first dataset\")\nplt.ylabel('Throughput (mb/s)')\nplt.xlabel('Latency (ms)')\nplt.axis([0, 25, 0, 25])\nplt.show()\n\n\n\n\n\n\nGaussian Distribution\n\\[\np(x ; \\mu,\\sigma ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}} \\exp \\left( - \\frac{(x - \\mu)^2}{2 \\sigma ^2} \\right)\n\\]\n\\[\n\\text{(Univariate Gaussian Distribution)}\n\\]\n\\[\np(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{k/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right)\n\\]\n\\[\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n    \\sigma_{1}^2 & \\sigma_{12} & \\sigma_{13} & \\ldots & \\sigma_{1k} \\\\\n    \\sigma_{21} & \\sigma_{2}^2 & \\sigma_{23} & \\ldots & \\sigma_{2k} \\\\\n    \\sigma_{31} & \\sigma_{32} & \\sigma_{3}^2 & \\ldots & \\sigma_{3k} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\sigma_{k1} & \\sigma_{k2} & \\sigma_{k3} & \\ldots & \\sigma_{k}^2 \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\text{(Multivariate Gaussian Distribution)}\n\\]\n\ndef estimate_gaussian(X):\n\n    mu = np.mean(X, axis=0)\n    var = np.var(X, axis=0)\n\n    return mu, var\n\n\ndef univariate_gaussian(X, mu, var):\n\n    p = (1 / (np.sqrt(2 * np.pi * var))) * np.exp(-((X - mu) ** 2) / (2 * var))\n\n    return p\n\n\ndef multivariate_gaussian(X, mu, var):\n\n    k = len(mu)\n\n    if var.ndim == 1:\n        var = np.diag(var)\n\n    X = X - mu\n    p = (2 * np.pi)**(-k/2) * np.linalg.det(var)**(-0.5) * \\\n        np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))\n\n    return p\n\n\nmu, var = estimate_gaussian(X_train)\nprint(\"mu = \", mu)\nprint(\"var = \", var)\n\nmu =  [14.11222578 14.99771051]\nvar =  [1.83263141 1.70974533]\n\n\n\nprint(univariate_gaussian(X_train[246][0], mu[0], var[0]))\n\n0.28071685840987737\n\n\n\nX1, X2 = np.meshgrid(np.arange(0, 30, 0.5), np.arange(0, 30, 0.5))\nZ = multivariate_gaussian(np.stack([X1.ravel(), X2.ravel()], axis=1), mu, var)\nZ = Z.reshape(X1.shape)\n\nplt.plot(X_train[:, 0], X_train[:, 1], 'bx')\nplt.contour(X1, X2, Z, levels=10**(np.arange(-20., 1, 3)), linewidths=1)\n\nplt.title(\"The Gaussian contours of the distribution fit to the dataset\")\nplt.ylabel('Throughput (mb/s)')\nplt.xlabel('Latency (ms)')\nplt.show()\n\n\n\n\n\n\nSelecting threshold \\(\\epsilon\\)\n\\[\\begin{aligned} prec&=\\frac{tp}{tp+fp} \\\\ rec &=\\frac{tp}{tp+fn},\\end{aligned}\\] \\[F_1 = \\frac{2\\cdot prec \\cdot rec}{prec + rec}\\]\n\ndef select_threshold(y_val, p_val):\n    best_epsilon = 0\n    best_F1 = 0\n    F1 = 0\n\n    step_size = (max(p_val) - min(p_val)) / 1000\n\n    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n\n        predictions = (p_val &lt; epsilon)\n\n        tp = np.sum((predictions == 1) & (y_val == 1))\n        fp = np.sum((predictions == 1) & (y_val == 0))\n        fn = np.sum((predictions == 0) & (y_val == 1))\n\n        prec, rec = 0, 0\n\n        if (tp+fp) != 0:\n            prec = (tp)/(tp+fp)\n        if (tp+fn) != 0:\n            rec = (tp)/(tp+fn)\n\n        F1 = 0\n        if (prec+rec) != 0:\n            F1 = 2*prec*rec/(prec+rec)\n\n        if F1 &gt; best_F1:\n            best_F1 = F1\n            best_epsilon = epsilon\n\n    return best_epsilon, best_F1\n\n\n\nFinding Outliers\n\np = multivariate_gaussian(X_train, mu, var)\np_val = multivariate_gaussian(X_test, mu, var)\nepsilon, F1 = select_threshold(y_test, p_val)\n\nprint('Best epsilon found using cross-validation: %e' % epsilon)\nprint('Best F1 on Cross Validation Set: %f' % F1)\n\nBest epsilon found using cross-validation: 8.990853e-05\nBest F1 on Cross Validation Set: 0.875000\n\n\n\noutliers = p &lt; epsilon\n\nX1, X2 = np.meshgrid(np.arange(0, 30, 0.5), np.arange(0, 30, 0.5))\nZ = multivariate_gaussian(np.stack([X1.ravel(), X2.ravel()], axis=1), mu, var)\nZ = Z.reshape(X1.shape)\n\nplt.plot(X_train[:, 0], X_train[:, 1], 'bx')\nplt.contour(X1, X2, Z, levels=10**(np.arange(-20., 1, 3)), linewidths=1)\nplt.plot(X_train[outliers, 0], X_train[outliers, 1], 'ro',\n         markersize=10, markerfacecolor='none', markeredgewidth=2)\n\nplt.title(\"The Gaussian contours of the distribution fit to the dataset\")\nplt.ylabel('Throughput (mb/s)')\nplt.xlabel('Latency (ms)')\nplt.show()\n\nprint('# Anomalies found: %d' % sum(p &lt; epsilon))\n\n\n\n\n# Anomalies found: 6"
  },
  {
    "objectID": "blogs/Decision_Tress.html",
    "href": "blogs/Decision_Tress.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Importing Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nDataset\n\nX_train: For each example, contains 3 features:\n\nEar Shape (1 if pointy, 0 otherwise)\nFace Shape (1 if round, 0 otherwise)\nWhiskers (1 if present, 0 otherwise)\n\ny_train: Whether the animal is a cat\n\n1 if the animal is a cat\n0 otherwise\n\n\n\nX_train = np.array([[1, 1, 1],\n                    [0, 0, 1],\n                    [0, 1, 0],\n                    [1, 0, 1],\n                    [1, 1, 1],\n                    [1, 1, 0],\n                    [0, 0, 0],\n                    [1, 1, 0],\n                    [0, 1, 0],\n                    [0, 1, 0]])\n\ny_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])\n\n\n\nEntropy Function\n\\[H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)\\]\n\ndef entropy(p):\n    if p == 0 or p == 1:\n        return 0\n    else:\n        return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n\np_values = np.linspace(0.000, 1.000, 100)\n\nentropy_values = [entropy(p) for p in p_values]\n\nplt.figure(figsize=(8, 6))\nplt.plot(p_values, entropy_values, label='Entropy', color='b')\nplt.xlabel('p')\nplt.ylabel('Entropy')\nplt.title('Entropy vs. p')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\nInformation Gain\n\\[\n\\text {Information Gain }= H(p^{\\text{node}}) - (w^{\\text{left}} H(p^{\\text{left}}) + w^{\\text{right}} H(p^{\\text{right}}))\n\\]\n\ndef split_indices(X, node_indices, feature):\n    left_indices = []\n    right_indices = []\n\n    for i in node_indices:\n        if X[i][feature] == 1:\n            left_indices.append(i)\n        else:\n            right_indices.append(i)\n\n    return left_indices, right_indices\n\n\ndef weighted_entropy(X, y, node_indices, index_feature):\n    weighted_entropy = 0\n\n    left_indices, right_indices = split_indices(X, node_indices, index_feature)\n    w_left = len(left_indices) / len(X)\n    w_right = len(right_indices) / len(X)\n    p_left=0\n    p_right=0\n\n    if len(left_indices)&gt;0:\n        p_left = sum(y[left_indices]) / len(left_indices)\n        \n    if len(right_indices)&gt;0:\n        p_right = sum(y[right_indices]) / len(right_indices)\n    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n\n    return weighted_entropy\n\n\ndef information_gain(X, y, node_indices, index_feature):\n    X_node, y_node = X[node_indices], y[node_indices]\n    information_gain = 0\n\n    p_node = sum(y_node[y_node == 1]) / len(y_node)\n    node_entropy = entropy(p_node)\n\n    weighted_entropy_temp = weighted_entropy(X, y, node_indices, index_feature)\n    information_gain = node_entropy - weighted_entropy_temp\n\n    return information_gain\n\n\nnode_indices = [0, 3, 4, 5, 7]\n\nfor i, feature_name in enumerate(['Ear Shape', 'Face Shape', 'Whiskers']):\n    i_gain = information_gain(X_train, y_train, node_indices, i)\n    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")\n\nFeature: Ear Shape, information gain if we split the root node using this feature: 0.36\nFeature: Face Shape, information gain if we split the root node using this feature: 0.72\nFeature: Whiskers, information gain if we split the root node using this feature: 0.45\n\n\n\n\nMaking the Tree\n\ndef get_best_split(X, y, node_indices):\n    num_features = X.shape[1]\n    best_feature = -1\n    max_info_gain = 0\n\n    for feature in range(num_features):\n        info_gain = information_gain(X, y, node_indices, feature)\n        if info_gain &gt; max_info_gain:\n            max_info_gain = info_gain\n            best_feature = feature\n\n    return best_feature\n\n\ndef build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth, tree):\n\n    if current_depth == max_depth:\n        formatting = \" \"*current_depth + \"-\"*current_depth\n        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n        return\n   \n\n    best_feature = get_best_split(X, y, node_indices) \n    \n    formatting = \"-\"*current_depth\n    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n    \n\n    left_indices, right_indices = split_indices(X, node_indices, best_feature)\n    tree.append((left_indices, right_indices, best_feature))\n    \n    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree)\n    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree)\n    return tree\n\n\ntree = []\nbuild_tree_recursive(X_train, y_train, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"Root\", max_depth=2, current_depth=0, tree=tree)\n\n Depth 0, Root: Split on feature: 0\n- Depth 1, Left: Split on feature: 1\n  -- Left leaf node with indices [0, 4, 5, 7]\n  -- Right leaf node with indices [3]\n- Depth 1, Right: Split on feature: 2\n  -- Left leaf node with indices [1]\n  -- Right leaf node with indices [2, 6, 8, 9]\n\n\n[([0, 3, 4, 5, 7], [1, 2, 6, 8, 9], 0),\n ([0, 4, 5, 7], [3], 1),\n ([1], [2, 6, 8, 9], 2)]"
  },
  {
    "objectID": "blogs/KMeans.html",
    "href": "blogs/KMeans.html",
    "title": "K Means",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "blogs/KMeans.html#kmeans-on-dataset",
    "href": "blogs/KMeans.html#kmeans-on-dataset",
    "title": "K Means",
    "section": "KMeans on Dataset",
    "text": "KMeans on Dataset\n\nData\n\ny = load_digits()\nx = y.data\n\n\nprint(y.data.shape)\nprint(y.target.shape)\n\n(1797, 64)\n(1797,)\n\n\n\n\nClosest Centroids\n\ndef find_closest_centroids(X, centroids):\n\n    idx = np.zeros(X.shape[0], dtype=int)\n    distances = np.linalg.norm(X[:, np.newaxis, :]-centroids, axis=-1)\n    idx = np.argmin(distances, axis=1)\n\n    return idx\n\n\n\nNew Centroids\n\ndef compute_centroids(X, idx, centroids, k):\n    for i in range(k):\n        centroids[i] = np.mean(X[idx == i], axis=0)\n    return centroids\n\n\n\nCost Fxn\n\ndef calculate_cost(X, idx, centroids):\n    cost = 0\n    for i in range(len(centroids)):\n        cluster_points = X[idx == i]\n        centroid = centroids[i]\n        squared_distances = np.sum((cluster_points - centroid) ** 2)\n        cost += squared_distances\n    return cost\n\n\n\nKMeans\n\ndef run_kMeans(X, max_iters, k):\n    n, d = X.shape\n    centroids = X[np.random.choice(n, k, replace=False)]\n\n    print(f\"{'Iteration':&lt;10}{'Cost':&lt;15}\")\n\n    prev_cost=np.inf\n    cost=0\n\n    for i in range(max_iters):\n        if np.abs(prev_cost-cost)&lt;1e-5:\n            break\n        idx = find_closest_centroids(X, centroids)\n        prev_cost=cost\n        cost = calculate_cost(X, idx, centroids)\n\n        print(f\"{i + 1:&lt;10}{cost:&lt;15}\")\n\n        centroids = compute_centroids(X, idx, centroids, k)\n\n    return centroids, idx\n\n\n\nAccuracy\n\na, b = run_kMeans(x, 500, 10)\nprint(\"Accuracy =\", 100*np.mean(y.target == b) ,\"%\")\n\nIteration Cost           \n1         2312326.0      \n2         1367849.205211381\n3         1272514.7623466342\n4         1230713.3819844497\n5         1217124.8773493604\n6         1214638.5685729042\n7         1214183.403014829\n8         1214039.6629300476\n9         1213981.741130655\n10        1213950.3799817525\n11        1213908.3568474432\n12        1213896.1627396639\n13        1213875.909452301\n14        1213848.8851707997\n15        1213820.8739280729\n16        1213820.8739280729\nAccuracy = 19.47690595436839 %"
  },
  {
    "objectID": "blogs/KMeans.html#image-compression-with-k-means",
    "href": "blogs/KMeans.html#image-compression-with-k-means",
    "title": "K Means",
    "section": "Image compression with K-means",
    "text": "Image compression with K-means\n\nOriginal Image\n\noriginal_img = plt.imread('img.jpg')\nplt.imshow(original_img)\nplt.show()\n\n\n\n\n\n# Gives the blue intensity of the pixel at row 50 and column 33.\noriginal_img[50, 33, 2]\n\n3\n\n\n\nprint(\"Shape of original_img is:\", original_img.shape)\n\nShape of original_img is: (477, 350, 3)\n\n\n\n\nKMeans\n\nX_img = np.reshape(\n    original_img, (original_img.shape[0] * original_img.shape[1], 3))\nprint(\"Shape of X_img is:\", X_img.shape)\n\nShape of X_img is: (166950, 3)\n\n\n\nmax_iter = 30\nk = 15\ncentroids, idx = run_kMeans(X_img, max_iter, k)\n\nIteration Cost           \n1         40518784       \n2         49937840       \n3         38376899       \n4         48464334       \n5         38469257       \n6         44709319       \n7         45348539       \n8         48159095       \n9         46984087       \n10        41761944       \n11        45318104       \n12        45340223       \n13        46252334       \n14        47408590       \n15        40701877       \n16        48466062       \n17        52976465       \n18        50425878       \n19        48800590       \n20        49841789       \n21        50815087       \n22        46825849       \n23        47234052       \n24        50064839       \n25        45833848       \n26        44100214       \n27        44035480       \n28        42271855       \n29        46069851       \n30        46009134       \n\n\n\n\nRGB Values plot with Centroids\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_img[:, 0], X_img[:, 1], X_img[:, 2], s=0.3)\nax.set_xlabel('R value')\nax.set_ylabel('G value')\nax.set_zlabel('B value')\nax.set_title('RGB Values Plot')\nax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], s=200, alpha=1, marker='*', c='red', zorder=2)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\nColors of Centroids\n\nfig, axs = plt.subplots(1, len(centroids))\nfor i in range(len(centroids)):\n    axs[i].imshow(centroids[i].reshape(1, 1, 3))\n    axs[i].axis('off')\nplt.show()\n\n\n\n\n\n\nDecomposed Image\n\nX_recovered = centroids[idx, :]\nX_recovered = np.reshape(X_recovered, original_img.shape)\n\nplt.imshow(X_recovered)\n\nplt.show()"
  },
  {
    "objectID": "blogs/Linear_Regression_for_Classification.html",
    "href": "blogs/Linear_Regression_for_Classification.html",
    "title": "Linear Regression For Classification",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import cm\n\n\n\nDataset\n\nx_train = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\ny_train = np.array([0, 0, 0, 1, 0, 1, 0, 1, 1])\n\n\npos = y_train == 1\nneg = y_train == 0\n\nplt.figure(figsize=(6, 4))\n\nplt.scatter(x_train[pos], y_train[pos],\n            marker='x', s=80, c='red', label=\"y=1\")\nplt.scatter(x_train[neg], y_train[neg], marker='o',\n            s=100, label=\"y=0\", facecolors='blue', edgecolors='black', linewidth=1)\nplt.ylim(-0.08, 1.1)\nplt.ylabel('y', fontsize=12)\nplt.xlabel('x', fontsize=12)\nplt.title('One Variable Plot')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nUsing Linear Regression\n\nx_train = x_train.reshape(-1, 1)\ny_train = y_train.reshape(-1, 1)\n\nmodel = LinearRegression()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_train)\n\ny_line = np.array([0.5] * len(x_train))\n\nx_intersection = (0.5 - model.intercept_[0]) / model.coef_[0]\n\n\nplt.figure(figsize=(12, 6))\nplt.scatter(x_train[pos], y_train[pos], marker='x', s=80, c='red', label=\"y=1\")\nplt.scatter(x_train[neg], y_train[neg], marker='o', s=100,\n            label=\"y=0\", facecolors='blue', edgecolors='black', linewidth=1)\n\nplt.plot(x_train, y_pred, color='red', label='Linear Regression Line')\nplt.plot([x_intersection, x_intersection],\n         [-0.2, 1.2], color='green', linestyle='--')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.annotate('←', xy=(x_intersection, 0.5), xytext=(x_intersection - 0.5, 0.6), fontsize=25, color='blue')\nplt.annotate('Y=0', xy=(x_intersection, 0.5), xytext=(x_intersection - 0.5, 0.5), fontsize=15, color='blue')\nplt.annotate('→', xy=(x_intersection, 0.5), xytext=(x_intersection + 0.1, 0.6),fontsize=25, color='red')\nplt.annotate('Y=1', xy=(x_intersection, 0.5), xytext=(x_intersection + 0.1, 0.5), fontsize=15, color='red')\nplt.show()\n\n\n\n\n\n\nLogistic Square Error Cost\n\ndef compute_cost_logistic_sq_err(X, y, w, b):\n    z = np.dot(X, w) + b\n    f_wb = 1 / (1 + np.exp(-z))\n    cost = np.mean((f_wb - y) ** 2) / 2\n    return cost\n\n\nwx, by = np.meshgrid(np.linspace(-6, 12, 50),\n                     np.linspace(10, -20, 40))\npoints = np.c_[wx.ravel(), by.ravel()]\ncost = np.zeros(points.shape[0])\n\nfor i in range(points.shape[0]):\n    w, b = points[i]\n    cost[i] = compute_cost_logistic_sq_err(x_train, y_train, w, b)\n\ncost = cost.reshape(wx.shape)\n\nfig = plt.figure(figsize=(18, 15))\nax = fig.add_subplot(1, 1, 1, projection='3d')\nax.plot_surface(wx, by, cost, alpha=0.6, cmap=cm.jet)\nax.set_xlabel('w', fontsize=16)\nax.set_ylabel('b', fontsize=16)\nax.set_zlabel(\"Cost\", rotation=90, fontsize=16)\nax.set_title('\"Logistic\" Squared Error Cost vs (w, b)')\n\nplt.show()"
  },
  {
    "objectID": "blogs/Linear_Regression_With_One_Variable.html",
    "href": "blogs/Linear_Regression_With_One_Variable.html",
    "title": "Linear Regression With One Variable",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\n\nSome Plotting Fxn\n\ndef plt_house_x(X, y, f_wb=None, ax=None):\n    ax.scatter(X, y, marker='x', c='r', label=\"Actual Value\")\n    ax.set_ylabel(\"Y-data\")\n    ax.set_xlabel(\"X-data\")\n    ax.plot(X, f_wb, label=\"Our Prediction\")\n    ax.legend()\n\n\ndef mk_cost_lines(x, y, w, b, ax):\n    label = 'Cost of point'\n    addedbreak = False\n    for p in zip(x, y):\n        f_wb_p = w*p[0]+b\n        c_p = ((f_wb_p - p[1])**2)/2\n        c_p_txt = c_p\n        ax.vlines(p[0], p[1], f_wb_p, lw=3, ls='dotted', label=label)\n        label = ''\n        cxy = [p[0], p[1] + (f_wb_p-p[1])/2]\n        ax.annotate(f'{c_p_txt:0.0f}', xy=cxy, xycoords='data',\n                    xytext=(5, 0), textcoords='offset points')\n\n\ndef plt_stationary(x_train, y_train, w0, b):\n    fig, ax = plt.subplots(1, 2, figsize=(9, 8))\n    fig.canvas.toolbar_position = 'top'\n\n    w_range = np.array([200-300., 200+300])\n    b_range = np.array([50-300., 50+300])\n    b_space = np.linspace(*b_range, 100)\n    w_space = np.linspace(*w_range, 100)\n\n    tmp_b, tmp_w = np.meshgrid(b_space, w_space)\n    z = np.zeros_like(tmp_b)\n    for i in range(tmp_w.shape[0]):\n        for j in range(tmp_w.shape[1]):\n            z[i, j] = cost_fxn(x_train, y_train, tmp_w[i][j], tmp_b[i][j])\n            if z[i, j] == 0:\n                z[i, j] = 1e-6\n\n    f_wb = np.dot(x_train, w0) + b\n    mk_cost_lines(x_train, y_train, w0, b, ax[0])\n    plt_house_x(x_train, y_train, f_wb=f_wb, ax=ax[0])\n\n    CS = ax[1].contour(tmp_w, tmp_b, np.log(z), levels=12,\n                       linewidths=2, alpha=0.7)\n    ax[1].set_title('Cost(w,b) [Contour Plot]')\n    ax[1].set_xlabel('w', fontsize=10)\n    ax[1].set_ylabel('b', fontsize=10)\n    ax[1].set_xlim(w_range)\n    ax[1].set_ylim(b_range)\n    cscat = ax[1].scatter(w0, b, s=100, zorder=10,)\n    chline = ax[1].hlines(b, ax[1].get_xlim()[0], w0,\n                          lw=4, ls='dotted')\n    cvline = ax[1].vlines(w0, ax[1].get_ylim()[0], b,\n                          lw=4, ls='dotted')\n    fig.tight_layout()\n    return fig, ax, [cscat, chline, cvline]\n\n\ndef soup_bowl(x_train, y_train, w0=200, b0=-100):\n    fig = plt.figure(figsize=(10, 10))\n\n    ax = fig.add_subplot(111, projection='3d')\n    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_rotate_label(False)\n    ax.view_init(30, -30)\n\n    w = np.linspace(-100, 300, 100)\n    b = np.linspace(-200, 300, 100)\n\n    z = np.zeros((len(w), len(b)))\n    for i in range(len(w)):\n        for j in range(len(b)):\n            z[i, j] = cost_fxn(x_train, y_train, w[i], b[j])\n\n    W, B = np.meshgrid(w, b)\n\n    ax.plot_surface(W, B, z, cmap=\"Spectral_r\", alpha=0.7, antialiased=False)\n    ax.plot_wireframe(W, B, z, color='k', alpha=0.1)\n    ax.set_xlabel(\"$w$\")\n    ax.set_ylabel(\"$b$\")\n    ax.set_zlabel(\"$J(w,b)$\", rotation=90)\n    ax.set_title(\"$J(w,b)$\", size=15)\n    cscat = ax.scatter(w0, b0, s=100, color='red')\n\n    plt.show()\n\n\n\nDataset\n\nx_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\ny_train = np.array([250, 300, 480, 430, 630, 730])\nw = 200\nb = -100\n\n\n\nFinding Function f_wb\n\\[\nf_{w,b}(x^{(i)}) = wx^{(i)} + b\n\\]\n\ndef fxn(x, w, b):\n    f_wb = w * x + b\n\n    return f_wb\n\n\n\nFinding Cost Function\n\\[J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\]\n\ndef cost_fxn(x, y, w, b):\n    m = x.shape[0]\n\n    f_wb = fxn(x, w, b)\n    cost = (f_wb - y) ** 2\n    total_cost = (1 / (2 * m)) * np.sum(cost)\n\n    return total_cost\n\n\n\nSome Plots -&gt;\n\nOriginal w, b\n\nfig, ax, dyn_items = plt_stationary(x_train, y_train, w, b)\nsoup_bowl(x_train, y_train, w, b)\n\n\n\n\n\n\n\n\n\n\nFinding dJ/dw and dJ/db\n\\[\n\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\\\\n  \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\\\\n\\]\n\ndef compute_gradient(x, y, w, b):\n    m = x.shape[0]\n    a = fxn(x, w, b) - y\n    dj_dw = (np.dot(a, x))/m\n    dj_db = np.sum(a)/m\n    return dj_dw, dj_db\n\n\n\nGradient Descent\n\\[\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}  \\; \\newline\nb &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}\\] where, parameters \\(w\\), \\(b\\) are updated simultaneously.\n\ndef gradient_descent(x, y, w, b, alpha, num_iters):\n    J_history = []\n    p_history = []\n\n    for i in range(num_iters + 1):\n        dj_dw, dj_db = compute_gradient(x, y, w, b)\n        w -= alpha * dj_dw\n        b -= alpha * dj_db\n\n        cost = cost_fxn(x, y, w, b)\n        J_history.append(cost)\n        p_history.append((w, b))\n\n        if i % math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4}: Cost {cost:.2e}, w: {w}, b: {b}\")\n\n    return w, b, J_history, p_history\n\n\niterations = 10000\ntmp_alpha = 1.0e-2\n\nw_final, b_final, J_hist, p_hist = gradient_descent(\n    x_train, y_train, w, b, tmp_alpha, iterations)\nprint(f\"(w,b) found by gradient descent: {w_final},{b_final}\")\n\n\nf_wb = fxn(x_train, w_final, b_final)\nprint(\"Cost is\", cost_fxn(x_train, y_train, w_final, b_final))\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\nax.set_title(\"Cost vs. iteration (end)\")\nax.set_ylabel('Cost')\nax.set_xlabel('iteration step')\nplt.show()\n\nIteration    0: Cost 8.46e+03, w: 202.80833333333334, b: -98.76666666666667\nIteration 1000: Cost 1.80e+03, w: 223.52137552092964, b: -32.28296188836681\nIteration 2000: Cost 1.75e+03, w: 215.18089273879156, b: -11.838434218472974\nIteration 3000: Cost 1.74e+03, w: 211.75363820423198, b: -3.4374097221896087\nIteration 4000: Cost 1.74e+03, w: 210.3453176127893, b: 0.014722492685807966\nIteration 5000: Cost 1.74e+03, w: 209.76661332681294, b: 1.4332657708600276\nIteration 6000: Cost 1.74e+03, w: 209.5288133168987, b: 2.0161707428604965\nIteration 7000: Cost 1.74e+03, w: 209.43109701154987, b: 2.255696890289678\nIteration 8000: Cost 1.74e+03, w: 209.39094362242898, b: 2.3541224966202314\nIteration 9000: Cost 1.74e+03, w: 209.37444387193037, b: 2.394567350284724\nIteration 10000: Cost 1.74e+03, w: 209.3676638273943, b: 2.4111868688115714\n(w,b) found by gradient descent: 209.3676638273943,2.4111868688115714\nCost is 1735.8832116012204\n\n\n\n\n\n\n\nSome Plots -&gt;\n\nFinal w, b\n\nfig, ax, dyn_items = plt_stationary(x_train, y_train, w_final, b_final)\nsoup_bowl(x_train, y_train, w_final, b_final)\n\n\n\n\n\n\n\n\n\n\nRegularized Linear Regression\n\nFinding Cost Fxn\n\\[J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\]\n\ndef cost_fxn_regularized(X, y, w, b, lambda_=1):\n    cost = cost_fxn(X, y, w, b)\n    cost += np.sum(w ** 2)\n    return cost\n\n\n\nFinding dJ/dw and dJ/db\n\\[\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n\\end{align*}\\]\n\ndef compute_gradient_regularized(X, y, w, b, lambda_):\n    m = X.shape[0]\n    dj_dw, dj_db = compute_gradient(X, y, w, b)\n\n    dj_dw += (lambda_ / m) * w\n\n    return dj_db, dj_dw"
  },
  {
    "objectID": "blogs/PCA.html",
    "href": "blogs/PCA.html",
    "title": "Principal Component Analysis ( P.C.A. )",
    "section": "",
    "text": "Import Libraries\n\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nData\n\nX = np.array([[1, 1], [2, 1], [3, 2], [-1, -1], [-2, -1], [-3, -2]])\nplt.plot(X[:, 0], X[:, 1], 'ro')\nplt.show()\n\n\n\n\n\n\nPCA (1 component)\n\npca = PCA(n_components=1)\npca.fit(X)\n\nPCA(n_components=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=1)\n\n\n\nsum(pca.explained_variance_ratio_)\n\n0.9924428900898052\n\n\n\nX_transformed = pca.transform(X)\nprint(X_transformed)\n\n[[ 1.38340578]\n [ 2.22189802]\n [ 3.6053038 ]\n [-1.38340578]\n [-2.22189802]\n [-3.6053038 ]]\n\n\n\nX_reduced = pca.inverse_transform(X_transformed)\nprint(X_reduced)\nplt.plot(X_reduced[:, 0], X_reduced[:, 1], 'ro')\nplt.show()\n\n[[ 1.15997501  0.75383654]\n [ 1.86304424  1.21074232]\n [ 3.02301925  1.96457886]\n [-1.15997501 -0.75383654]\n [-1.86304424 -1.21074232]\n [-3.02301925 -1.96457886]]\n\n\n\n\n\n\n\nPCA (2 component)\n\npca = PCA(n_components=2)\npca.fit(X)\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=2)\n\n\n\nsum(pca.explained_variance_ratio_)\n\n1.0\n\n\n\nX_transformed = pca.transform(X)\nprint(X_transformed)\n\n[[ 1.38340578  0.2935787 ]\n [ 2.22189802 -0.25133484]\n [ 3.6053038   0.04224385]\n [-1.38340578 -0.2935787 ]\n [-2.22189802  0.25133484]\n [-3.6053038  -0.04224385]]\n\n\n\n\nMy PCA\n\ndef My_PCA(X, k):\n\n    X_std = (X - np.mean(X, axis=0))\n\n    cov_mat = np.cov(X_std.T)\n\n    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\n    eigenvectors = eig_vecs[:, np.argsort(eig_vals)[::-1]]\n\n    pca_mat = eigenvectors[:, :k]\n\n    pca = np.dot(X_std, pca_mat)\n\n    return pca\n\n\nprint(My_PCA(X, 1))\n\n[[ 1.38340578]\n [ 2.22189802]\n [ 3.6053038 ]\n [-1.38340578]\n [-2.22189802]\n [-3.6053038 ]]"
  },
  {
    "objectID": "blogs/SImple_Neutral_Network.html#using-tensorflow",
    "href": "blogs/SImple_Neutral_Network.html#using-tensorflow",
    "title": "Simple Neural Network",
    "section": "Using Tensorflow",
    "text": "Using Tensorflow\n\nSetting the Pipeline\n\nmodel = Sequential(\n    [\n        tf.keras.Input(shape=(2,)),\n        Dense(3, activation='sigmoid', name='layer1'),\n        Dense(1, activation='sigmoid', name='layer2')\n    ]\n)\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n layer1 (Dense)              (None, 3)                 9         \n                                                                 \n layer2 (Dense)              (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 13 (52.00 Byte)\nTrainable params: 13 (52.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nRandom Initial Weights\n\nW1, b1 = model.get_layer(\"layer1\").get_weights()        # Random Generation\nW2, b2 = model.get_layer(\"layer2\").get_weights()        # Random Generation\nprint(\"W1:\\n\", W1, \"\\nb1:\", b1)\nprint(\"W2:\\n\", W2, \"\\nb2:\", b2)\n\nW1:\n [[ 1.0034692  -0.5364701  -0.61829925]\n [-0.45948058 -0.48300928 -0.2479465 ]] \nb1: [0. 0. 0.]\nW2:\n [[ 0.795092  ]\n [ 0.50782764]\n [-0.8502825 ]] \nb2: [0.]\n\n\n\n\nFitting the model\n\nmodel.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n)\n\nmodel.fit(\n    Xt, Yt,\n    epochs=10,\n)\n\nEpoch 1/10\nWARNING:tensorflow:From C:\\Users\\vrajs\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n6250/6250 [==============================] - 11s 2ms/step - loss: 0.2032\nEpoch 2/10\n6250/6250 [==============================] - 10s 2ms/step - loss: 0.0970\nEpoch 3/10\n6250/6250 [==============================] - 11s 2ms/step - loss: 0.0321\nEpoch 4/10\n6250/6250 [==============================] - 12s 2ms/step - loss: 0.0198\nEpoch 5/10\n6250/6250 [==============================] - 10s 2ms/step - loss: 0.0146\nEpoch 6/10\n6250/6250 [==============================] - 10s 2ms/step - loss: 0.0116\nEpoch 7/10\n6250/6250 [==============================] - 10s 2ms/step - loss: 0.0095\nEpoch 8/10\n6250/6250 [==============================] - 11s 2ms/step - loss: 0.0080\nEpoch 9/10\n6250/6250 [==============================] - 11s 2ms/step - loss: 0.0068\nEpoch 10/10\n6250/6250 [==============================] - 10s 2ms/step - loss: 0.0059\n\n\n&lt;keras.src.callbacks.History at 0x278f700ac20&gt;\n\n\n\n\nFinal Weights\n\nW1, b1 = model.get_layer(\"layer1\").get_weights()\nW2, b2 = model.get_layer(\"layer2\").get_weights()\nprint(\"W1:\\n\", W1, \"\\nb1:\", b1)\nprint(\"W2:\\n\", W2, \"\\nb2:\", b2)\n\nW1:\n [[13.708808   -0.04309544 13.338246  ]\n [ 0.2935187  10.918727   11.044739  ]] \nb1: [14.286802 13.310424  1.352872]\nW2:\n [[ 27.669281]\n [ 27.278173]\n [-37.544296]] \nb2: [-35.913513]\n\n\n\n\nPredictions\n\nrng = np.random.default_rng(0)\nX = rng.random(400).reshape(-1, 2)\nX[:, 1] = X[:, 1] * 4 + 11.5\nX[:, 0] = X[:, 0] * (285-150) + 150\nY = np.zeros(len(X))\n\ni = 0\nfor t, d in X:\n    y = -3/(260-175)*t + 21\n    if (t &gt; 180 and t &lt; 260 and d &gt; 12 and d &lt; 15 and d &lt;= y):\n        Y[i] = 1\n    else:\n        Y[i] = 0\n    i += 1\n\nX_testn = norm_l(X)\npredictions = model.predict(X_testn)\n\npredictions_thresholded = (predictions &gt; 0.5).astype(int).reshape(-1,)\n\nplt.scatter(X[predictions_thresholded == 1, 0], X[predictions_thresholded == 1, 1], s=70, marker='x',\n            linewidth=3,  c='red', label=\"Good\")\nplt.scatter(X[predictions_thresholded == 0, 0],\n            X[predictions_thresholded == 0, 1], s=50, marker='o', label=\"Bad\")\n\ntr = np.linspace(175, 260, 50)\nplt.plot(tr, (-3 / 85) * tr + 21, linewidth=1)\nplt.axhline(y=12, linewidth=1)\nplt.axvline(x=175, linewidth=1)\n\nplt.xlabel(\"Feature 1\", size=12)\nplt.ylabel(\"Feature 2\", size=12)\nplt.legend(loc='upper right')\n\nplt.show()\n\n7/7 [==============================] - 0s 1ms/step"
  },
  {
    "objectID": "blogs/SImple_Neutral_Network.html#using-numpy",
    "href": "blogs/SImple_Neutral_Network.html#using-numpy",
    "title": "Simple Neural Network",
    "section": "Using Numpy",
    "text": "Using Numpy\n\nSetting the Pipeline\n\ndef my_dense(a_in, W, b):\n    units = W.shape[1]\n    a_out = np.zeros(units)\n    for j in range(units):\n        w = W[:, j]\n        z = np.dot(w, a_in) + b[j]\n        a_out[j] = 1/(1+np.exp(-z))\n    return (a_out)\n\ndef my_dense(a_in, W, b):\n    a_out=1/(1+(np.exp(-(np.matmul(a_in,W)+b))))\n    return (a_out)\n\n\ndef my_sequential(x, W1, b1, W2, b2):\n    a1 = my_dense(x,  W1, b1)\n    a2 = my_dense(a1, W2, b2)\n    return (a2)\n\n\nW1_tmp = np.array([[-8.93,  0.29, 12.9], [-0.1,  -7.32, 10.81]])\nb1_tmp = np.array([-9.82, -9.28,  0.96])\nW2_tmp = np.array([[-31.18], [-27.59], [-32.56]])\nb2_tmp = np.array([15.41])\n\n\n\nPrediction\n\ndef my_predict(X, W1, b1, W2, b2):\n    m = X.shape[0]\n    p = np.zeros((m, 1))\n    for i in range(m):\n        p[i, 0] = my_sequential(X[i], W1, b1, W2, b2)\n    return (p)\n\n\nrng = np.random.default_rng(0)\nX = rng.random(400).reshape(-1, 2)\nX[:, 1] = X[:, 1] * 4 + 11.5\nX[:, 0] = X[:, 0] * (285-150) + 150\nY = np.zeros(len(X))\n\ni = 0\nfor t, d in X:\n    y = -3/(260-175)*t + 21\n    if (t &gt; 180 and t &lt; 260 and d &gt; 12 and d &lt; 15 and d &lt;= y):\n        Y[i] = 1\n    else:\n        Y[i] = 0\n    i += 1\n\nX_tstn = norm_l(X)\npredictions = my_predict(X_tstn, W1_tmp, b1_tmp, W2_tmp, b2_tmp)\n\npredictions_thresholded = (predictions &gt; 0.5).astype(int).reshape(-1,)\n\nplt.scatter(X[predictions_thresholded == 1, 0], X[predictions_thresholded == 1, 1], s=70, marker='x',\n            linewidth=3,  c='red', label=\"Good\")\nplt.scatter(X[predictions_thresholded == 0, 0],\n            X[predictions_thresholded == 0, 1], s=50, marker='o', label=\"Bad\")\n\ntr = np.linspace(175, 260, 50)\nplt.plot(tr, (-3 / 85) * tr + 21, linewidth=1)\nplt.axhline(y=12, linewidth=1)\nplt.axvline(x=175, linewidth=1)\n\nplt.xlabel(\"Feature 1\", size=12)\nplt.ylabel(\"Feature 2\", size=12)\nplt.legend(loc='upper right')\n\nplt.show()\n\nC:\\Users\\vrajs\\AppData\\Local\\Temp\\ipykernel_14280\\3457910700.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  p[i, 0] = my_sequential(X[i], W1, b1, W2, b2)"
  },
  {
    "objectID": "blogs/State_Action_Value_Function.html",
    "href": "blogs/State_Action_Value_Function.html",
    "title": "State Action Value Function",
    "section": "",
    "text": "Import Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nData\n\nnum_states = 8\nnum_actions = 2\n\nterminal_left_reward = 100\nterminal_right_reward = 40\neach_step_reward = 0\n\ngamma = 0.5                 # Discount factor\n\nmisstep_prob = 0            # Probability of going in the wrong direction\n\n\n\nQ Values\n\ndef generate_rewards(num_states, each_step_reward, terminal_left_reward, terminal_right_reward):\n\n    rewards = [each_step_reward] * num_states\n    rewards[0] = terminal_left_reward\n    rewards[-1] = terminal_right_reward\n    \n    return rewards \n\ndef generate_transition_prob(num_states, num_actions, misstep_prob = 0):\n    \n    p = np.zeros((num_states, num_actions, num_states))\n    \n    for i in range(num_states):        \n        if i != 0:\n            p[i, 0, i-1] = 1 - misstep_prob\n            p[i, 1, i-1] = misstep_prob\n            \n        if i != num_states - 1:\n            p[i, 1, i+1] = 1  - misstep_prob\n            p[i, 0, i+1] = misstep_prob\n        \n    # Terminal States\n    p[0] = np.zeros((num_actions, num_states))\n    p[-1] = np.zeros((num_actions, num_states))\n    \n    return p\n\ndef calculate_Q_value(num_states, rewards, transition_prob, gamma, V_states, state, action):\n    q_sa = rewards[state] + gamma * sum([transition_prob[state, action, sp] * V_states[sp] for sp in range(num_states)])\n    return q_sa\n\ndef evaluate_policy(num_states, rewards, transition_prob, gamma, policy):\n    max_policy_eval = 10000\n    threshold = 1e-10\n    \n    V = np.zeros(num_states)\n    \n    for i in range(max_policy_eval):\n        delta = 0\n        for s in range(num_states):\n            v = V[s]\n            V[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V, s, policy[s])\n            delta = max(delta, abs(v - V[s]))\n                       \n        if delta &lt; threshold:\n            break\n            \n    return V\n\n\ndef calculate_Q_values(num_states, rewards, transition_prob, gamma, optimal_policy):\n    # Left and then optimal policy\n    q_left_star = np.zeros(num_states)\n\n    # Right and optimal policy\n    q_right_star = np.zeros(num_states)\n    \n    V_star =  evaluate_policy(num_states, rewards, transition_prob, gamma, optimal_policy)\n\n    for s in range(num_states):\n        q_left_star[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V_star, s, 0)\n        q_right_star[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V_star, s, 1)\n        \n    return q_left_star, q_right_star\n\n\n\nOptimize Policy\n\ndef improve_policy(num_states, num_actions, rewards, transition_prob, gamma, V, policy):\n    policy_stable = True\n    \n    for s in range(num_states):\n        q_best = V[s]\n        for a in range(num_actions):\n            q_sa = calculate_Q_value(num_states, rewards, transition_prob, gamma, V, s, a)\n            if q_sa &gt; q_best and policy[s] != a:\n                policy[s] = a\n                q_best = q_sa\n                policy_stable = False\n    \n    return policy, policy_stable\n\n\ndef get_optimal_policy(num_states, num_actions, rewards, transition_prob, gamma):\n    optimal_policy = np.zeros(num_states, dtype=int)\n    max_policy_iter = 10000 \n\n    for i in range(max_policy_iter):\n        policy_stable = True\n\n        V = evaluate_policy(num_states, rewards, transition_prob, gamma, optimal_policy)\n        optimal_policy, policy_stable = improve_policy(num_states, num_actions, rewards, transition_prob, gamma, V, optimal_policy)\n\n        if policy_stable:\n            break\n            \n    return optimal_policy, V\n\n\n\nVisualization\n\ndef plot_optimal_policy_return(num_states, optimal_policy, rewards, V):\n    actions = [r\"$\\leftarrow$\" if a == 0 else r\"$\\rightarrow$\" for a in optimal_policy]\n    actions[0] = \"\"\n    actions[-1] = \"\"\n    \n    fig, ax = plt.subplots(figsize=(2*num_states,2))\n\n    for i in range(num_states):\n        ax.text(i+0.5, 0.5, actions[i], fontsize=32, ha=\"center\", va=\"center\", color=\"orange\")\n        ax.text(i+0.5, 0.25, rewards[i], fontsize=16, ha=\"center\", va=\"center\", color=\"black\")\n        ax.text(i+0.5, 0.75, round(V[i],2), fontsize=16, ha=\"center\", va=\"center\", color=\"firebrick\")\n        ax.axvline(i, color=\"black\")\n    ax.set_xlim([0, num_states])\n    ax.set_ylim([0, 1])\n\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.tick_params(axis='both', which='both', length=0)\n    ax.set_title(\"Optimal policy\",fontsize = 16)\n\n\ndef plot_q_values(num_states, q_left_star, q_right_star, rewards):\n    fig, ax = plt.subplots(figsize=(3*num_states,2))\n\n    for i in range(num_states):\n        ax.text(i+0.2, 0.6, round(q_left_star[i],2), fontsize=16, ha=\"center\", va=\"center\", color=\"firebrick\")\n        ax.text(i+0.8, 0.6, round(q_right_star[i],2), fontsize=16, ha=\"center\", va=\"center\", color=\"firebrick\")\n\n        ax.text(i+0.5, 0.25, rewards[i], fontsize=20, ha=\"center\", va=\"center\", color=\"black\")\n        ax.axvline(i, color=\"black\")\n    ax.set_xlim([0, num_states])\n    ax.set_ylim([0, 1])\n\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.tick_params(axis='both', which='both', length=0)\n    ax.set_title(\"Q(s,a)\",fontsize = 16)\n\n\ndef generate_visualization(num_states,num_actions,terminal_left_reward, terminal_right_reward, each_step_reward, gamma, misstep_prob):\n\n    rewards = generate_rewards(num_states, each_step_reward, terminal_left_reward, terminal_right_reward)\n    transition_prob = generate_transition_prob(num_states, num_actions, misstep_prob)\n    \n    optimal_policy, V = get_optimal_policy(num_states, num_actions, rewards, transition_prob, gamma)\n    q_left_star, q_right_star = calculate_Q_values(num_states, rewards, transition_prob, gamma, optimal_policy)\n    \n    plot_optimal_policy_return(num_states, optimal_policy, rewards, V)\n    plot_q_values(num_states, q_left_star, q_right_star, rewards)\n\n\ngenerate_visualization(num_states,num_actions,terminal_left_reward, terminal_right_reward, each_step_reward, gamma, misstep_prob)"
  }
]