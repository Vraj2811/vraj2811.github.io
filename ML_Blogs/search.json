[
  {
    "objectID": "Supervised_ML/Linear_Regression_With_One_Variable.html",
    "href": "Supervised_ML/Linear_Regression_With_One_Variable.html",
    "title": "Linear Regression With One Variable",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\n\nSome Plotting Fxn\n\ndef plt_house_x(X, y, f_wb=None, ax=None):\n    ax.scatter(X, y, marker='x', c='r', label=\"Actual Value\")\n    ax.set_ylabel(\"Y-data\")\n    ax.set_xlabel(\"X-data\")\n    ax.plot(X, f_wb, label=\"Our Prediction\")\n    ax.legend()\n\n\ndef mk_cost_lines(x, y, w, b, ax):\n    label = 'Cost of point'\n    addedbreak = False\n    for p in zip(x, y):\n        f_wb_p = w*p[0]+b\n        c_p = ((f_wb_p - p[1])**2)/2\n        c_p_txt = c_p\n        ax.vlines(p[0], p[1], f_wb_p, lw=3, ls='dotted', label=label)\n        label = ''\n        cxy = [p[0], p[1] + (f_wb_p-p[1])/2]\n        ax.annotate(f'{c_p_txt:0.0f}', xy=cxy, xycoords='data',\n                    xytext=(5, 0), textcoords='offset points')\n\n\ndef plt_stationary(x_train, y_train, w0, b):\n    fig, ax = plt.subplots(1, 2, figsize=(9, 8))\n    fig.canvas.toolbar_position = 'top'\n\n    w_range = np.array([200-300., 200+300])\n    b_range = np.array([50-300., 50+300])\n    b_space = np.linspace(*b_range, 100)\n    w_space = np.linspace(*w_range, 100)\n\n    tmp_b, tmp_w = np.meshgrid(b_space, w_space)\n    z = np.zeros_like(tmp_b)\n    for i in range(tmp_w.shape[0]):\n        for j in range(tmp_w.shape[1]):\n            z[i, j] = cost_fxn(x_train, y_train, tmp_w[i][j], tmp_b[i][j])\n            if z[i, j] == 0:\n                z[i, j] = 1e-6\n\n    f_wb = np.dot(x_train, w0) + b\n    mk_cost_lines(x_train, y_train, w0, b, ax[0])\n    plt_house_x(x_train, y_train, f_wb=f_wb, ax=ax[0])\n\n    CS = ax[1].contour(tmp_w, tmp_b, np.log(z), levels=12,\n                       linewidths=2, alpha=0.7)\n    ax[1].set_title('Cost(w,b) [Contour Plot]')\n    ax[1].set_xlabel('w', fontsize=10)\n    ax[1].set_ylabel('b', fontsize=10)\n    ax[1].set_xlim(w_range)\n    ax[1].set_ylim(b_range)\n    cscat = ax[1].scatter(w0, b, s=100, zorder=10,)\n    chline = ax[1].hlines(b, ax[1].get_xlim()[0], w0,\n                          lw=4, ls='dotted')\n    cvline = ax[1].vlines(w0, ax[1].get_ylim()[0], b,\n                          lw=4, ls='dotted')\n    fig.tight_layout()\n    return fig, ax, [cscat, chline, cvline]\n\n\ndef soup_bowl(x_train, y_train, w0=200, b0=-100):\n    fig = plt.figure(figsize=(10, 10))\n\n    # Plot configuration\n    ax = fig.add_subplot(111, projection='3d')\n    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_rotate_label(False)\n    ax.view_init(30, -30)\n\n    w = np.linspace(-100, 300, 100)\n    b = np.linspace(-200, 300, 100)\n\n    z = np.zeros((len(w), len(b)))\n    for i in range(len(w)):\n        for j in range(len(b)):\n            z[i, j] = cost_fxn(x_train, y_train, w[i], b[j])\n\n    W, B = np.meshgrid(w, b)\n\n    ax.plot_surface(W, B, z, cmap=\"Spectral_r\", alpha=0.7, antialiased=False)\n    ax.plot_wireframe(W, B, z, color='k', alpha=0.1)\n    ax.set_xlabel(\"$w$\")\n    ax.set_ylabel(\"$b$\")\n    ax.set_zlabel(\"$J(w,b)$\", rotation=90)\n    ax.set_title(\"$J(w,b)$\", size=15)\n    cscat = ax.scatter(w0, b0, s=100, color='red')\n\n    plt.show()\n\n\n\nDataset\n\nx_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\ny_train = np.array([250, 300, 480, 430, 630, 730])\nw = 200\nb = -100\n\n\n\nFinding Function f_wb\n\\[\nf_{w,b}(x^{(i)}) = wx^{(i)} + b\n\\]\n\ndef fxn(x, w, b):\n    f_wb = w * x + b\n    \n    return f_wb\n\n\n\nFinding Cost Function\n\\[J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\]\n\ndef cost_fxn(x, y, w, b):\n    m = x.shape[0]\n\n    f_wb = fxn(x,w,b)\n    cost = (f_wb - y) ** 2\n    total_cost = (1 / (2 * m)) * np.sum(cost)\n\n    return total_cost\n\n\nSome Plots\n\n\nOriginal w, b\n\nfig, ax, dyn_items = plt_stationary(x_train, y_train, w, b)\nsoup_bowl(x_train, y_train, w, b)\n\n\n\n\n\n\n\n\n\n\nFinding dJ/dw and dJ/db\n\\[\n\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\\\\n  \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\\\\n\\]\n\ndef compute_gradient(x, y, w, b):\n    m = x.shape[0]\n    a = fxn(x, w, b) - y\n    dj_dw = (np.dot(a, x))/m\n    dj_db = np.sum(a)/m\n    return dj_dw, dj_db\n\n\n\nGradient Descent\n\\[\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}  \\; \\newline\nb &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}\\] where, parameters \\(w\\), \\(b\\) are updated simultaneously.\n\ndef gradient_descent(x, y, w, b, alpha, num_iters):\n    J_history = []\n    p_history = []\n\n    for i in range(num_iters + 1):\n        dj_dw, dj_db = compute_gradient(x, y, w, b)\n        w -= alpha * dj_dw\n        b -= alpha * dj_db\n\n        cost = cost_fxn(x, y, w, b)\n        J_history.append(cost)\n        p_history.append((w, b))\n\n        if i % math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4}: Cost {cost:.2e}, w: {w}, b: {b}\")\n\n    return w, b, J_history, p_history\n\n\niterations = 10000\ntmp_alpha = 1.0e-2\n\nw_final, b_final, J_hist, p_hist = gradient_descent(\n    x_train, y_train, w, b, tmp_alpha, iterations)\nprint(f\"(w,b) found by gradient descent: {w_final},{b_final}\")\n\n\nf_wb = fxn(x_train, w_final, b_final)\nprint(\"Cost is\", cost_fxn(x_train, y_train, w_final, b_final))\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\nax.set_title(\"Cost vs. iteration (end)\")\nax.set_ylabel('Cost')\nax.set_xlabel('iteration step')\nplt.show()\n\nIteration    0: Cost 8.46e+03, w: 202.80833333333334, b: -98.76666666666667\nIteration 1000: Cost 1.80e+03, w: 223.52137552092964, b: -32.28296188836681\nIteration 2000: Cost 1.75e+03, w: 215.18089273879156, b: -11.838434218472974\nIteration 3000: Cost 1.74e+03, w: 211.75363820423198, b: -3.4374097221896087\nIteration 4000: Cost 1.74e+03, w: 210.3453176127893, b: 0.014722492685807966\nIteration 5000: Cost 1.74e+03, w: 209.76661332681294, b: 1.4332657708600276\nIteration 6000: Cost 1.74e+03, w: 209.5288133168987, b: 2.0161707428604965\nIteration 7000: Cost 1.74e+03, w: 209.43109701154987, b: 2.255696890289678\nIteration 8000: Cost 1.74e+03, w: 209.39094362242898, b: 2.3541224966202314\nIteration 9000: Cost 1.74e+03, w: 209.37444387193037, b: 2.394567350284724\nIteration 10000: Cost 1.74e+03, w: 209.3676638273943, b: 2.4111868688115714\n(w,b) found by gradient descent: 209.3676638273943,2.4111868688115714\nCost is 1735.8832116012204\n\n\n\n\n\n\nSome Plots\n\n\nFinal w, b\n\nfig, ax, dyn_items = plt_stationary(x_train, y_train, w_final, b_final)\nsoup_bowl(x_train, y_train, w_final, b_final)\n\n\n\n\n\n\n\n\n\n\nRegularized Linear Regression\n\nFinding Cost Fxn\n\\[J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\]\n\ndef cost_fxn_regularized(X, y, w, b, lambda_ = 1):\n    cost=cost_fxn(X, y, w, b)\n    cost += np.sum(w ** 2)\n    return cost\n\n\n\nFinding dJ/dw and dJ/db\n\\[\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n\\end{align*}\\]\n\ndef compute_gradient_regularized(X, y, w, b, lambda_):\n    m = X.shape[0]\n    dj_dw, dj_db = compute_gradient(X, y, w, b)\n\n    dj_dw += (lambda_ / m) * w\n\n    return dj_db, dj_dw"
  },
  {
    "objectID": "Supervised_ML/Polynomial_Regression.html",
    "href": "Supervised_ML/Polynomial_Regression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\n\nPolynomial Features\n\ndef create_polynomial_features(X, degree):\n    X_poly = np.ones((X.shape[0], 1))\n    for d in range(1, degree + 1):\n        X_poly = np.hstack((X_poly, X.reshape(-1, 1) ** d))\n    return X_poly\n\ndef solve_normal_equation(X, y):\n    return np.linalg.inv(X.T @ X) @ X.T @ y\n\ndef predict(X, coefficients):\n    return X @ coefficients\n\n\n\nDataset 1\n\nx = np.arange(0, 20, 1)\ny = 1 + x**2\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\")\nplt.legend()\nplt.title(\"y=1+x^2\")\nplt.show()\n\n\n\n\n\n\nPredictions for Different Degrees\n\nDegree = 0\n\ndegree = 0\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nDegree = 1\n\ndegree = 1\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nDegree = 2\n\ndegree = 2\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nDataset 2\n\nx = np.arange(0,20,1)\ny = np.cos(x/2)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\")\nplt.legend()\nplt.title(\"y=Cos(x/2)\")\nplt.show()\n\n\n\n\n\n\nPredictions for Different Degrees\n\nDegree = 1\n\ndegree = 1\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nDegree = 4\n\ndegree = 4\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nDegree = 9\n\ndegree = 9\n\n\nX_poly = create_polynomial_features(x, degree)\n\ncoefficients = solve_normal_equation(X_poly, y)\n\ny_pred = predict(X_poly, coefficients)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, color='gray', label='Actual')\nplt.plot(x, y_pred, color='blue', label='Train Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Supervised_ML/Linear_Regression_With_Multiple_Variable.html",
    "href": "Supervised_ML/Linear_Regression_With_Multiple_Variable.html",
    "title": "Linear Regression With Multiple Variable",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\n\nDataset\n\nx_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\ny_train = np.array([460, 232, 178])\nb = -100\nw = np.array([10, -10, 50, -20])\n\n\n\nFinding Function f_wb\n\\[ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\] \\[ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b   \\]\n\ndef fxn(x, w, b):\n    f_wb = np.dot(x,w) + b\n    \n    return f_wb\n\n\n\nFinding Cost Function\n\\[J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\]\n\ndef cost_fxn(x, y, w, b):\n    m = x.shape[0]\n\n    f_wb = fxn(x, w, b)\n    cost = (f_wb - y) ** 2\n    total_cost = (1 / (2 * m)) * np.sum(cost)\n\n    return total_cost\n\n\n\nFinding dJ/dw and dJ/db\n\\[\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\\\\n\\]\n\ndef compute_gradient(x, y, w, b):\n    m = x.shape[0]\n    a = fxn(x, w, b) - y\n    dj_dw = (np.dot(a, x))/m\n    dj_db = np.sum(a)/m\n    return dj_dw, dj_db\n\n\n\nGradient Descent\n\\[\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  \\; & \\text{for j = 0..n-1}\\newline\n&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}\\]\n\ndef gradient_descent(x, y, w, b, alpha, num_iters):\n    J_history = []\n    p_history = []\n\n    for i in range(num_iters + 1):\n        dj_dw, dj_db = compute_gradient(x, y, w, b)\n        w = w - alpha * dj_dw\n        b = b - alpha * dj_db\n\n        cost = cost_fxn(x, y, w, b)\n        J_history.append(cost)\n        p_history.append((w, b))\n\n        if i % math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4}: Cost {cost:.2e}, w: {w}, b: {b}\")\n\n    return w, b, J_history, p_history\n\n\niterations = 100000\ntmp_alpha = 5.0e-7\n\nw_final, b_final, J_hist, p_hist = gradient_descent(\n    x_train, y_train, w, b, tmp_alpha, iterations)\nprint(f\"(w,b) found by gradient descent: {w_final},{b_final}\")\n\n\nf_wb = fxn(x_train, w_final, b_final)\nprint(\"Cost is\", cost_fxn(x_train, y_train, w_final, b_final))\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\nax.set_title(\"Cost vs. iteration (end)\")\nax.set_ylabel('Cost')\nax.set_xlabel('iteration step')\nplt.show()\n\nIteration    0: Cost 3.85e+06, w: [ -1.03240533 -10.02538967  49.99110867 -20.27834   ], b: -100.00670833333334\nIteration 10000: Cost 8.71e+03, w: [  0.58117979 -10.02160299  50.21100127 -13.2564183 ], b: -99.75760289212182\nIteration 20000: Cost 4.63e+03, w: [  0.467721   -10.00429679  50.3020335   -8.78158635], b: -99.59764948351305\nIteration 30000: Cost 2.95e+03, w: [ 0.39492336 -9.97570356 50.311448   -5.91048217], b: -99.49414563632797\nIteration 40000: Cost 2.26e+03, w: [ 0.34819099 -9.93989568 50.26860068 -4.06742547], b: -99.42682903727315\nIteration 50000: Cost 1.97e+03, w: [ 0.31816743 -9.89948366 50.19230962 -2.88338545], b: -99.38271084836177\nIteration 60000: Cost 1.85e+03, w: [ 0.29885486 -9.85614089 50.09463778 -2.12180075], b: -99.35346477833464\nIteration 70000: Cost 1.80e+03, w: [ 0.28640849 -9.81094006 49.98331786 -1.63102743], b: -99.33375338803842\nIteration 80000: Cost 1.77e+03, w: [ 0.27836366 -9.76456876 49.86330674 -1.3138565 ], b: -99.32015519486049\nIteration 90000: Cost 1.76e+03, w: [ 0.27314044 -9.71746776 49.73778185 -1.10797371], b: -99.31047690994058\nIteration 100000: Cost 1.75e+03, w: [ 0.26972607 -9.66991956 49.60877999 -0.97343345], b: -99.3033125586677\n(w,b) found by gradient descent: [ 0.26972607 -9.66991956 49.60877999 -0.97343345],-99.3033125586677\nCost is 1753.0909002940225"
  },
  {
    "objectID": "Supervised_ML/Linear_Regression_for_Classification.html",
    "href": "Supervised_ML/Linear_Regression_for_Classification.html",
    "title": "Linear Regression For Classification",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import cm\n\n\n\nDataset\n\nx_train = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\ny_train = np.array([0, 0, 0, 1, 0, 1, 0, 1, 1])\n\n\npos = y_train == 1\nneg = y_train == 0\n\nplt.figure(figsize=(6, 4))\n\nplt.scatter(x_train[pos], y_train[pos],\n            marker='x', s=80, c='red', label=\"y=1\")\nplt.scatter(x_train[neg], y_train[neg], marker='o',\n            s=100, label=\"y=0\", facecolors='blue', edgecolors='black', linewidth=1)\nplt.ylim(-0.08, 1.1)\nplt.ylabel('y', fontsize=12)\nplt.xlabel('x', fontsize=12)\nplt.title('One Variable Plot')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nUsing Linear Regression\n\nx_train = x_train.reshape(-1, 1)\ny_train = y_train.reshape(-1, 1)\n\nmodel = LinearRegression()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_train)\n\ny_line = np.array([0.5] * len(x_train))\n\nx_intersection = (0.5 - model.intercept_[0]) / model.coef_[0]\n\n\nplt.figure(figsize=(12, 6))\nplt.scatter(x_train[pos], y_train[pos], marker='x', s=80, c='red', label=\"y=1\")\nplt.scatter(x_train[neg], y_train[neg], marker='o', s=100,\n            label=\"y=0\", facecolors='blue', edgecolors='black', linewidth=1)\n\nplt.plot(x_train, y_pred, color='red', label='Linear Regression Line')\nplt.plot([x_intersection, x_intersection],\n         [-0.2, 1.2], color='green', linestyle='--')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.annotate('←', xy=(x_intersection, 0.5), xytext=(x_intersection - 0.5, 0.6), fontsize=25, color='blue')\nplt.annotate('Y=0', xy=(x_intersection, 0.5), xytext=(x_intersection - 0.5, 0.5), fontsize=15, color='blue')\nplt.annotate('→', xy=(x_intersection, 0.5), xytext=(x_intersection + 0.1, 0.6),fontsize=25, color='red')\nplt.annotate('Y=1', xy=(x_intersection, 0.5), xytext=(x_intersection + 0.1, 0.5), fontsize=15, color='red')\nplt.show()\n\n\n\n\n\n\nLogistic Square Error Cost\n\ndef compute_cost_logistic_sq_err(X, y, w, b):\n    z = np.dot(X, w) + b\n    f_wb = 1 / (1 + np.exp(-z))\n    cost = np.mean((f_wb - y) ** 2) / 2\n    return cost\n\n\nwx, by = np.meshgrid(np.linspace(-6, 12, 50),\n                     np.linspace(10, -20, 40))\npoints = np.c_[wx.ravel(), by.ravel()]\ncost = np.zeros(points.shape[0])\n\nfor i in range(points.shape[0]):\n    w, b = points[i]\n    cost[i] = compute_cost_logistic_sq_err(x_train, y_train, w, b)\n\ncost = cost.reshape(wx.shape)\n\nfig = plt.figure(figsize=(18, 15))\nax = fig.add_subplot(1, 1, 1, projection='3d')\nax.plot_surface(wx, by, cost, alpha=0.6, cmap=cm.jet)\nax.set_xlabel('w', fontsize=16)\nax.set_ylabel('b', fontsize=16)\nax.set_zlabel(\"Cost\", rotation=90, fontsize=16)\nax.set_title('\"Logistic\" Squared Error Cost vs (w, b)')\n\nplt.show()"
  },
  {
    "objectID": "Supervised_ML/Logistic_Regression.html",
    "href": "Supervised_ML/Logistic_Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Libraries Required\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom matplotlib import cm\n\n\n\nSome Plotting Functions\n\ndef soup_bowl(x_train, y_train, w, b):\n    wx, by = np.meshgrid(np.linspace(-6, 12, 50),\n                         np.linspace(10, -20, 40))\n    points = np.c_[wx.ravel(), by.ravel()]\n    cost = np.zeros(points.shape[0])\n\n    for i in range(points.shape[0]):\n        w_i, b_i = points[i]\n        cost[i] = cost_fxn(x_train, y_train, w_i, b_i)\n\n    cost = cost.reshape(wx.shape)\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(1, 1, 1, projection='3d')\n    ax.plot_surface(wx, by, cost, alpha=0.6, cmap=cm.jet)\n    ax.set_xlabel('w', fontsize=20)\n    ax.set_ylabel('b', fontsize=20)\n    ax.set_zlabel(\"Cost\", rotation=90, fontsize=20)\n    ax.set_title('\"Logistic\" Squared Error Cost vs (w, b)', fontsize=20)\n    cscat = ax.scatter(w, b, s=100, color='red')\n    plt.tight_layout()\n\n    plt.show()\n\n\n\nDataset\n\nx_train = np.array([0, 1, 2, 3, 4, 5]).reshape(-1, 1)\ny_train = np.array([0,  0, 0, 1, 1, 1])\n\nw = np.array([5])\nb = 10\n\n\npos = y_train == 1\nneg = y_train == 0\n\nplt.figure(figsize=(6, 4))\n\nplt.scatter(x_train[pos], y_train[pos],\n            marker='x', s=80, c='red', label=\"y=1\")\nplt.scatter(x_train[neg], y_train[neg], marker='o',\n            s=100, label=\"y=0\", facecolors='blue', edgecolors='black', linewidth=1)\nplt.ylim(-0.08, 1.1)\nplt.ylabel('y', fontsize=12)\nplt.xlabel('x', fontsize=12)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nSigmoid Fxn\n\\[g(z) = \\frac{1}{1+e^{-z}}\\]\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nz = np.linspace(-8, 8, 100)\n\nsigmoid_values = sigmoid(z)\n\nplt.figure(figsize=(6, 4))\nplt.plot(z, sigmoid_values, label='Sigmoid Function')\nplt.xlabel('z')\nplt.ylabel('sigmoid(z)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nFinding Function f_wb\n\\[ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\]\n\ndef fxn(x, w, b):\n    f_wb = sigmoid(np.dot(x, w) + b)\n    return f_wb\n\n\nif len(w) == 1:\n    fxn1 = fxn(x_train, w, b)\n\n    plt.scatter(x_train, y_train, color=\"red\")\n    plt.plot(x_train, fxn1)\n    plt.show()\n\n\n\n\n\n\nDecision Boundary\n\\[\\mathbf{w} \\cdot \\mathbf{x} = w_0 x_0 + w_1 x_1 = 0\\]\n\nDataset\n\nx_db = np.array([[0.5, 1.5], [1, 1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_db = np.array([0, 0, 0, 1, 1, 1]).reshape(-1, 1)\n\n\nx0 = np.arange(0, 6)\n\nx1 = 3 - x0\nfig, ax = plt.subplots(1, 1, figsize=(5, 4))\nax.plot(x0, x1, c=\"b\")\nax.axis([0, 4, 0, 3.5])\n\nax.fill_between(x0, x1, alpha=0.2)\n\npos = y_db == 1\nneg = y_db == 0\npos = pos.reshape(-1,)\nneg = neg.reshape(-1,)\n\nplt.scatter(x_db[neg, 0], x_db[neg, 1], marker='o', s=80,\n            label=\"neg_label\", facecolors='none', edgecolors=\"blue\", lw=3)\nplt.scatter(x_db[pos, 0], x_db[pos, 1], marker='x',\n            s=80, c='red', label=\"pos_label\")\nplt.show()\nplt.show()\n\n\n\n\n\n\n\nLoss Fxn\n\\[\n  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n  \\end{cases}\n\\]\n\\[= -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\]\n\ndef loss(x, y, w, b):\n    a = fxn(x, w, b)\n    epsilon = 1e-15  # Small constant to avoid taking log(0)\n\n    loss = -y * math.log(a + epsilon) - (1 - y) * math.log(1 - a + epsilon)\n    return loss\n\n\n\nCost Fxn\n\\[ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right]\\]\n\ndef cost_fxn(X, y, w, b):\n\n    m = X.shape[0]\n    cost = 0\n    for i in range(m):\n        cost += loss(X[i], y[i], w, b)\n\n    cost = cost / m\n    return cost\n\n\nSome Plots\n\nsoup_bowl(x_train, y_train, w, b)\n\n\n\n\n\n\n\nFinding dJ/dw and dJ/db\n\\[\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n\\end{align*}\\]\n\ndef compute_gradient(x, y, w, b):\n    dj_dw = 0\n    dj_db = 0\n    m = x.shape[0]\n    a = fxn(x, w, b) - y\n    dj_dw = (np.dot(a, x)) / m\n    dj_db = np.sum(a) / m\n    return dj_dw, dj_db\n\n\n\nGradient Descent\n\\[\\begin{align*}\n&\\text{repeat until convergence:} \\; \\lbrace \\\\\n&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j := 0..n-1} \\\\\n&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n&\\rbrace\n\\end{align*}\\]\n\ndef gradient_descent(x, y, w, b, alpha, num_iters):\n    J_history = []\n    p_history = []\n\n    for i in range(num_iters+1):\n        dj_dw, dj_db = compute_gradient(x, y, w, b)\n        b = b - alpha * dj_db\n        w = w - alpha * dj_dw\n        J_history.append(cost_fxn(x, y, w, b))\n        p_history.append([w, b])\n        if i % math.ceil(num_iters/10) == 0:\n            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}, w: {w}, b:{b}\")\n\n    return w, b, J_history, p_history\n\n\niterations = 10000\ntmp_alpha = 1.0e-1\n\n\nw_final, b_final, J_hist, p_hist = gradient_descent(\n    x_train, y_train, w, b, tmp_alpha, iterations)\nprint(f\"(w,b) found by gradient descent: ({w_final},{b_final})\")\n\n\nf_wb = fxn(x_train, w_final, b_final)\nprint(\"Cost is\", cost_fxn(x_train, y_train, w_final, b_final))\n\nIteration    0: Cost 7.45e+00, w: [4.95000001], b:9.950000761764102\nIteration 1000: Cost 1.32e-01, w: [1.94357846], b:-4.532187811486597\nIteration 2000: Cost 8.42e-02, w: [2.71585819], b:-6.535471646001672\nIteration 3000: Cost 6.46e-02, w: [3.22521954], b:-7.834466336781398\nIteration 4000: Cost 5.30e-02, w: [3.62055449], b:-8.83606124058392\nIteration 5000: Cost 4.51e-02, w: [3.94791911], b:-9.662597001748475\nIteration 6000: Cost 3.92e-02, w: [4.22882495], b:-10.37034842110977\nIteration 7000: Cost 3.48e-02, w: [4.47546538], b:-10.990900553853033\nIteration 8000: Cost 3.12e-02, w: [4.69557844], b:-11.544161480692926\nIteration 9000: Cost 2.83e-02, w: [4.89444965], b:-12.043663029887693\nIteration 10000: Cost 2.59e-02, w: [5.07588043], b:-12.499101969930441\n(w,b) found by gradient descent: ([5.07588043],-12.499101969930441)\nCost is 0.025934093960807036\n\n\n\nSome Plots\n\nif len(w) == 1:\n    fxn2 = fxn(x_train, w_final, b_final)\n\n    plt.scatter(x_train, y_train, color=\"red\")\n    plt.plot(x_train, fxn2)\n    plt.show()\n\n\n\n\n\nsoup_bowl(x_train, y_train, w_final, b_final)\n\n\n\n\n\n\n\nRegularized Linear Regression\nFinding Cost Fxn\n\\[J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\]\nFinding dJ/dw and dJ/db\n\\[\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})\n\\end{align*}\\]"
  }
]